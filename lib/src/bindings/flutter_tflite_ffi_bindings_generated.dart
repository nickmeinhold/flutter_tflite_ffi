// ignore_for_file: always_specify_types, unused_field, unused_element
// ignore_for_file: sort_constructors_first
// ignore_for_file: require_trailing_commas
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names
// ignore_for_file: no_leading_underscores_for_local_identifiers
// ignore_for_file: use_raw_strings

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// Bindings for TensorFlow Lite C API.
///
/// Regenerate bindings with `dart run ffigen --config ffigen.yaml`.
///
class FlutterTfliteFfiBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  FlutterTfliteFfiBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  FlutterTfliteFfiBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  /// Returns a new TfLiteOperator instance.
  ///
  /// The returned TfLiteOperator instance represents a definition
  /// of an operator with the identity (builtin_code/custom_name and
  /// version) specified by the parameters, but with all callbacks initially
  /// unset.
  ///
  /// Evaluation of any operation using this operator will be done using
  /// the "prepare" and "invoke" callbacks, which can be set using
  /// `TfLiteOperatorSetPrepare` and
  /// `TfLiteOperatorSetInvoke`, or for async execution
  /// the "prepare", "eval", and "wait" callbacks of the `TfLiteAsyncKernel`,
  /// which can be set using `TfLiteOperatorSetAsyncKernel`.
  /// If the relevant callbacks are not set, then such evaluation will result
  /// in an error status.  So normally any use of this function should be followed
  /// by appropriate calls to set those callbacks.
  ///
  /// \note The caller retains ownership and should ensure that
  /// the lifetime of the `TfLiteOperator` must be at least as long as
  /// the lifetime of any `TfLiteInterpreter` or `tflite::Interpreter` that it is
  /// used in.
  ///
  /// \param builtin_code Enumeration code specifying which builtin operator this
  /// defines, or `TfLiteBuiltinCustom` to define a custom op.
  /// \param custom_name  Name of the custom op, or `nullptr` for a builtin op.
  /// If `custom_name` is non-null, then `builtin_code` should
  /// be `TfLiteBuiltinCustom`.
  /// \param version      Version of the op.  See
  /// https://www.tensorflow.org/lite/guide/ops_version
  /// \param user_data    Opaque pointer passed to the operator's callbacks set
  /// with functions such as `TfLiteOperatorSetXXXWithData`.
  /// The user is expected to manage the memory pointed by
  /// this field and the lifetime of that memory should extend
  /// at least from the call to `TfLiteOperatorCreate`
  /// to the invocation of the callback set with
  /// `TfLiteOperatorSetFreeWithData`.
  ///
  /// \return a newly created TfLiteOperator on success, or a nullptr on failure
  ffi.Pointer<TfLiteOperator> TfLiteOperatorCreate(
    TfLiteBuiltinOperator builtin_code,
    ffi.Pointer<ffi.Char> custom_name,
    int version,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _TfLiteOperatorCreate(
      builtin_code.value,
      custom_name,
      version,
      user_data,
    );
  }

  late final _TfLiteOperatorCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteOperator> Function(
              ffi.UnsignedInt,
              ffi.Pointer<ffi.Char>,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('TfLiteOperatorCreate');
  late final _TfLiteOperatorCreate = _TfLiteOperatorCreatePtr.asFunction<
      ffi.Pointer<TfLiteOperator> Function(
          int, ffi.Pointer<ffi.Char>, int, ffi.Pointer<ffi.Void>)>();

  /// Destroys the TfLiteOperator instance.
  void TfLiteOperatorDelete(
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return _TfLiteOperatorDelete(
      registration,
    );
  }

  late final _TfLiteOperatorDeletePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteOperator>)>>(
      'TfLiteOperatorDelete');
  late final _TfLiteOperatorDelete = _TfLiteOperatorDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteOperator>)>();

  /// Return the builtin op code of the provided external 'registration'.
  TfLiteBuiltinOperator TfLiteOperatorGetBuiltInCode(
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return TfLiteBuiltinOperator.fromValue(_TfLiteOperatorGetBuiltInCode(
      registration,
    ));
  }

  late final _TfLiteOperatorGetBuiltInCodePtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteOperator>)>>('TfLiteOperatorGetBuiltInCode');
  late final _TfLiteOperatorGetBuiltInCode = _TfLiteOperatorGetBuiltInCodePtr
      .asFunction<int Function(ffi.Pointer<TfLiteOperator>)>();

  /// Returns the custom name of the provided 'registration'. The returned pointer
  /// will be non-null iff the op is a custom op.
  ffi.Pointer<ffi.Char> TfLiteOperatorGetCustomName(
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return _TfLiteOperatorGetCustomName(
      registration,
    );
  }

  late final _TfLiteOperatorGetCustomNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteOperator>)>>('TfLiteOperatorGetCustomName');
  late final _TfLiteOperatorGetCustomName =
      _TfLiteOperatorGetCustomNamePtr.asFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteOperator>)>();

  /// Return the OP version of the provided external 'registration'.  Return -1
  /// in case of error, or if the provided address is null.
  int TfLiteOperatorGetVersion(
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return _TfLiteOperatorGetVersion(
      registration,
    );
  }

  late final _TfLiteOperatorGetVersionPtr = _lookup<
          ffi.NativeFunction<ffi.Int Function(ffi.Pointer<TfLiteOperator>)>>(
      'TfLiteOperatorGetVersion');
  late final _TfLiteOperatorGetVersion = _TfLiteOperatorGetVersionPtr
      .asFunction<int Function(ffi.Pointer<TfLiteOperator>)>();

  /// Return the user data field of the provided external 'registration', or
  /// nullptr if none was set.
  ffi.Pointer<ffi.Void> TfLiteOperatorGetUserData(
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return _TfLiteOperatorGetUserData(
      registration,
    );
  }

  late final _TfLiteOperatorGetUserDataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<TfLiteOperator>)>>('TfLiteOperatorGetUserData');
  late final _TfLiteOperatorGetUserData =
      _TfLiteOperatorGetUserDataPtr.asFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteOperator>)>();

  /// Sets the initialization callback for the registration.
  ///
  /// The callback is called to initialize the op from serialized data.
  /// Please refer `init` of `TfLiteRegistration` for the detail.
  ///
  /// Deprecated: Use `TfLiteOperatorSetInitWithData`
  void TfLiteOperatorSetInit(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<ffi.Void> Function(
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Char> buffer,
                    ffi.Size length)>>
        init,
  ) {
    return _TfLiteOperatorSetInit(
      registration,
      init,
    );
  }

  late final _TfLiteOperatorSetInitPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<ffi.Void> Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<ffi.Char> buffer,
                          ffi.Size length)>>)>>('TfLiteOperatorSetInit');
  late final _TfLiteOperatorSetInit = _TfLiteOperatorSetInitPtr.asFunction<
      void Function(
          ffi.Pointer<TfLiteOperator>,
          ffi.Pointer<
              ffi.NativeFunction<
                  ffi.Pointer<ffi.Void> Function(
                      ffi.Pointer<TfLiteOpaqueContext> context,
                      ffi.Pointer<ffi.Char> buffer,
                      ffi.Size length)>>)>();

  /// Sets the initialization callback for the registration. The function returns
  /// an error upon failure.
  ///
  /// The callback is called to initialize the op from serialized data. The value
  /// passed in the `user_data` parameter is the value that was passed to
  /// `TfLiteOperatorCreate`.  Please refer `init` of `TfLiteRegistration`
  /// for the detail.
  TfLiteStatus TfLiteOperatorSetInitWithData(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<ffi.Void> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Char> buffer,
                    ffi.Size length)>>
        init,
  ) {
    return TfLiteStatus.fromValue(_TfLiteOperatorSetInitWithData(
      registration,
      init,
    ));
  }

  late final _TfLiteOperatorSetInitWithDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<ffi.Void> Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<ffi.Char> buffer,
                              ffi.Size length)>>)>>(
      'TfLiteOperatorSetInitWithData');
  late final _TfLiteOperatorSetInitWithData =
      _TfLiteOperatorSetInitWithDataPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<ffi.Void> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<ffi.Char> buffer,
                          ffi.Size length)>>)>();

  /// Sets the deallocation callback for the registration.
  ///
  /// This callback is called to deallocate the data returned by the init
  /// callback. The value passed in the `data` parameter is the value that was
  /// returned by the `init` callback. Please refer `free` of `TfLiteRegistration`
  /// for the detail.
  ///
  /// Deprecated: Use `TfLiteOperatorSetFreeWithData`
  void TfLiteOperatorSetFree(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Void> data)>>
        free,
  ) {
    return _TfLiteOperatorSetFree(
      registration,
      free,
    );
  }

  late final _TfLiteOperatorSetFreePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<ffi.Void> data)>>)>>(
      'TfLiteOperatorSetFree');
  late final _TfLiteOperatorSetFree = _TfLiteOperatorSetFreePtr.asFunction<
      void Function(
          ffi.Pointer<TfLiteOperator>,
          ffi.Pointer<
              ffi.NativeFunction<
                  ffi.Void Function(ffi.Pointer<TfLiteOpaqueContext> context,
                      ffi.Pointer<ffi.Void> data)>>)>();

  /// Sets the deallocation callback for the registration, similarly to
  /// `TfLiteOperatorSetFree`. The function returns an error upon failure.
  ///
  /// This callback is called to deallocate the data returned by the init
  /// callback. The value passed in the `data` parameter is the value that was
  /// returned by the `init` callback. The value passed in the `user_data`
  /// parameter is the value that was passed to `TfLiteOperatorCreate`.
  /// Please refer `free` of `TfLiteRegistration` for the detail.
  TfLiteStatus TfLiteOperatorSetFreeWithData(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<ffi.Void> data)>>
        free,
  ) {
    return TfLiteStatus.fromValue(_TfLiteOperatorSetFreeWithData(
      registration,
      free,
    ));
  }

  late final _TfLiteOperatorSetFreeWithDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<ffi.Void> data)>>)>>(
      'TfLiteOperatorSetFreeWithData');
  late final _TfLiteOperatorSetFreeWithData =
      _TfLiteOperatorSetFreeWithDataPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<ffi.Void> data)>>)>();

  /// Sets the preparation callback for the registration.
  ///
  /// The callback is called when the inputs of operator have been resized.
  /// Please refer `prepare` of `TfLiteRegistration` for the detail.
  ///
  /// Deprecated: Use `TfLiteOperatorSetPrepareWithData`
  void TfLiteOperatorSetPrepare(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.UnsignedInt Function(
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        prepare,
  ) {
    return _TfLiteOperatorSetPrepare(
      registration,
      prepare,
    );
  }

  late final _TfLiteOperatorSetPreparePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.UnsignedInt Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetPrepare');
  late final _TfLiteOperatorSetPrepare =
      _TfLiteOperatorSetPreparePtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.UnsignedInt Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the preparation callback for the registration. The function returns an
  /// error upon failure.
  ///
  /// The callback is called when the inputs of operator have been resized.  The
  /// value passed in the `user_data` parameter is the value that was passed to
  /// `TfLiteOperatorCreate`.  Please refer `prepare` of
  /// `TfLiteRegistration` for the detail.
  TfLiteStatus TfLiteOperatorSetPrepareWithData(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.UnsignedInt Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        prepare,
  ) {
    return TfLiteStatus.fromValue(_TfLiteOperatorSetPrepareWithData(
      registration,
      prepare,
    ));
  }

  late final _TfLiteOperatorSetPrepareWithDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.UnsignedInt Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetPrepareWithData');
  late final _TfLiteOperatorSetPrepareWithData =
      _TfLiteOperatorSetPrepareWithDataPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.UnsignedInt Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the invocation callback for the registration.
  ///
  /// The callback is called when the operator is executed.
  /// Please refer `invoke` of `TfLiteRegistration` for the detail.
  ///
  /// Deprecated: Use `TfLiteOperatorSetInvokeWithData`
  void TfLiteOperatorSetInvoke(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.UnsignedInt Function(
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        invoke,
  ) {
    return _TfLiteOperatorSetInvoke(
      registration,
      invoke,
    );
  }

  late final _TfLiteOperatorSetInvokePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.UnsignedInt Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetInvoke');
  late final _TfLiteOperatorSetInvoke = _TfLiteOperatorSetInvokePtr.asFunction<
      void Function(
          ffi.Pointer<TfLiteOperator>,
          ffi.Pointer<
              ffi.NativeFunction<
                  ffi.UnsignedInt Function(
                      ffi.Pointer<TfLiteOpaqueContext> context,
                      ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the invocation callback for the registration. The function returns an
  /// error upon failure.
  ///
  /// The callback is called when the operator is executed.  The value passed in
  /// the `user_data` parameter is the value that was passed to
  /// `TfLiteOperatorCreate`.  Please refer `invoke` of `TfLiteRegistration` for
  /// the detail.
  TfLiteStatus TfLiteOperatorSetInvokeWithData(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.UnsignedInt Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        invoke,
  ) {
    return TfLiteStatus.fromValue(_TfLiteOperatorSetInvokeWithData(
      registration,
      invoke,
    ));
  }

  late final _TfLiteOperatorSetInvokeWithDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.UnsignedInt Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetInvokeWithData');
  late final _TfLiteOperatorSetInvokeWithData =
      _TfLiteOperatorSetInvokeWithDataPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.UnsignedInt Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the async kernel accessor callback for the registration.
  ///
  /// The callback is called to retrieve the async kernel if the delegate supports
  /// it. If the delegate does not support async execution, either this function
  /// should not be called, or `async_kernel` needs to be nullptr.
  /// `node` is the delegate TfLiteNode created by `ModifyGraphWithDelegate`.
  /// Please refer `async_kernel` of `TfLiteRegistration` for the detail.
  ///
  /// \warning This is an experimental API and subject to change.
  /// Deprecated: Use `TfLiteOperatorSetAsyncKernelWithData`
  void TfLiteOperatorSetAsyncKernel(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteAsyncKernel> Function(
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        async_kernel,
  ) {
    return _TfLiteOperatorSetAsyncKernel(
      registration,
      async_kernel,
    );
  }

  late final _TfLiteOperatorSetAsyncKernelPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<TfLiteAsyncKernel> Function(
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetAsyncKernel');
  late final _TfLiteOperatorSetAsyncKernel =
      _TfLiteOperatorSetAsyncKernelPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteAsyncKernel> Function(
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the async kernel accessor callback for the registration. The function
  /// returns an error upon failure.
  ///
  /// The callback is called to retrieve the async kernel if the delegate supports
  /// it. If the delegate does not support async execution, either this function
  /// should not be called, or `async_kernel` needs to be nullptr.  `node` is the
  /// delegate TfLiteNode created by `ModifyGraphWithDelegate`.  The value passed
  /// in the `user_data` parameter is the value that was passed to
  /// `TfLiteOperatorCreate`.  Please refer `async_kernel` of `TfLiteRegistration`
  /// for the detail.
  ///
  /// \warning This is an experimental API and subject to change.
  TfLiteStatus TfLiteOperatorSetAsyncKernelWithData(
    ffi.Pointer<TfLiteOperator> registration,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Pointer<TfLiteAsyncKernel> Function(
                    ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<TfLiteOpaqueContext> context,
                    ffi.Pointer<TfLiteOpaqueNode> node)>>
        async_kernel,
  ) {
    return TfLiteStatus.fromValue(_TfLiteOperatorSetAsyncKernelWithData(
      registration,
      async_kernel,
    ));
  }

  late final _TfLiteOperatorSetAsyncKernelWithDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteOperator>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Pointer<TfLiteAsyncKernel> Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<TfLiteOpaqueContext> context,
                              ffi.Pointer<TfLiteOpaqueNode> node)>>)>>(
      'TfLiteOperatorSetAsyncKernelWithData');
  late final _TfLiteOperatorSetAsyncKernelWithData =
      _TfLiteOperatorSetAsyncKernelWithDataPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteOperator>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Pointer<TfLiteAsyncKernel> Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<TfLiteOpaqueContext> context,
                          ffi.Pointer<TfLiteOpaqueNode> node)>>)>();

  /// Sets the inplace_operator field of the external registration.
  ///
  /// This is a bitmask. Please refer to `inplace_operator` field of
  /// `TfLiteRegistration` for details.
  void TfLiteOperatorSetInplaceOperator(
    ffi.Pointer<TfLiteOperator> registration,
    int inplace_operator,
  ) {
    return _TfLiteOperatorSetInplaceOperator(
      registration,
      inplace_operator,
    );
  }

  late final _TfLiteOperatorSetInplaceOperatorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteOperator>,
              ffi.Uint64)>>('TfLiteOperatorSetInplaceOperator');
  late final _TfLiteOperatorSetInplaceOperator =
      _TfLiteOperatorSetInplaceOperatorPtr.asFunction<
          void Function(ffi.Pointer<TfLiteOperator>, int)>();

  /// --------------------------------------------------------------------------
  /// The TensorFlow Lite Runtime version.
  ///
  /// Returns a pointer to a statically allocated string that is the version
  /// number of the (potentially dynamically loaded) TF Lite Runtime library.
  /// TensorFlow Lite uses semantic versioning, and the return value should be
  /// in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
  /// e.g. "2.12.0" or "2.13.0-rc2".
  ffi.Pointer<ffi.Char> TfLiteVersion() {
    return _TfLiteVersion();
  }

  late final _TfLiteVersionPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'TfLiteVersion');
  late final _TfLiteVersion =
      _TfLiteVersionPtr.asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// --------------------------------------------------------------------------
  /// The TensorFlow Lite Extension APIs version.
  ///
  /// Returns a pointer to a statically allocated string that is the version
  /// number of the TF Lite Extension APIs supported by the (potentially
  /// dynamically loaded) TF Lite Runtime library.  The TF Lite "Extension APIs"
  /// are the APIs for extending TF Lite with custom ops and delegates.
  /// More specifically, this version number covers the (non-experimental)
  /// functionality documented in the following header files:
  ///
  /// * lite/c/c_api_opaque.h
  /// * lite/c/common.h
  /// * lite/c/builtin_op_data.h
  /// * lite/builtin_ops.h
  ///
  /// This version number uses semantic versioning, and the return value should
  /// be in semver 2 format <http://semver.org>, starting with MAJOR.MINOR.PATCH,
  /// e.g. "2.14.0" or "2.15.0-rc2".
  ffi.Pointer<ffi.Char> TfLiteExtensionApisVersion() {
    return _TfLiteExtensionApisVersion();
  }

  late final _TfLiteExtensionApisVersionPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'TfLiteExtensionApisVersion');
  late final _TfLiteExtensionApisVersion = _TfLiteExtensionApisVersionPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// The supported TensorFlow Lite model file Schema version.
  ///
  /// Returns the (major) version number of the Schema used for model
  /// files that is supported by the (potentially dynamically loaded)
  /// TensorFlow Lite Runtime.
  ///
  /// Model files using schema versions different to this may not be supported by
  /// the current version of the TF Lite Runtime.
  int TfLiteSchemaVersion() {
    return _TfLiteSchemaVersion();
  }

  late final _TfLiteSchemaVersionPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('TfLiteSchemaVersion');
  late final _TfLiteSchemaVersion =
      _TfLiteSchemaVersionPtr.asFunction<int Function()>();

  /// Returns a model from the provided buffer, or null on failure.
  ///
  /// \note The caller retains ownership of the `model_data` buffer and should
  /// ensure that the lifetime of the `model_data` buffer must be at least as long
  /// as the lifetime of the `TfLiteModel` and of any `TfLiteInterpreter` objects
  /// created from that `TfLiteModel`, and furthermore the contents of the
  /// `model_data` buffer must not be modified during that time."
  ffi.Pointer<TfLiteModel> TfLiteModelCreate(
    ffi.Pointer<ffi.Void> model_data,
    int model_size,
  ) {
    return _TfLiteModelCreate(
      model_data,
      model_size,
    );
  }

  late final _TfLiteModelCreatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Void>, ffi.Size)>>('TfLiteModelCreate');
  late final _TfLiteModelCreate = _TfLiteModelCreatePtr.asFunction<
      ffi.Pointer<TfLiteModel> Function(ffi.Pointer<ffi.Void>, int)>();

  /// Same as `TfLiteModelCreate` with customizble error reporter.
  /// * `reporter` takes the provided `user_data` object, as well as a C-style
  /// format string and arg list (see also vprintf).
  /// * `user_data` is optional. If non-null, it is owned by the client and must
  /// remain valid for the duration of the interpreter lifetime.
  ffi.Pointer<TfLiteModel> TfLiteModelCreateWithErrorReporter(
    ffi.Pointer<ffi.Void> model_data,
    int model_size,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> format, ffi.Pointer<ffi.Char> args)>>
        reporter,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _TfLiteModelCreateWithErrorReporter(
      model_data,
      model_size,
      reporter,
      user_data,
    );
  }

  late final _TfLiteModelCreateWithErrorReporterPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> format,
                          ffi.Pointer<ffi.Char> args)>>,
              ffi.Pointer<ffi.Void>)>>('TfLiteModelCreateWithErrorReporter');
  late final _TfLiteModelCreateWithErrorReporter =
      _TfLiteModelCreateWithErrorReporterPtr.asFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Void>,
              int,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> format,
                          ffi.Pointer<ffi.Char> args)>>,
              ffi.Pointer<ffi.Void>)>();

  /// Returns a model from the provided file, or null on failure.
  ///
  /// \note The file's contents must not be modified during the lifetime of the
  /// `TfLiteModel` or of any `TfLiteInterpreter` objects created from that
  /// `TfLiteModel`.
  ffi.Pointer<TfLiteModel> TfLiteModelCreateFromFile(
    ffi.Pointer<ffi.Char> model_path,
  ) {
    return _TfLiteModelCreateFromFile(
      model_path,
    );
  }

  late final _TfLiteModelCreateFromFilePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Char>)>>('TfLiteModelCreateFromFile');
  late final _TfLiteModelCreateFromFile = _TfLiteModelCreateFromFilePtr
      .asFunction<ffi.Pointer<TfLiteModel> Function(ffi.Pointer<ffi.Char>)>();

  /// Same as `TfLiteModelCreateFromFile` with customizble error reporter.
  /// * `reporter` takes the provided `user_data` object, as well as a C-style
  /// format string and arg list (see also vprintf).
  /// * `user_data` is optional. If non-null, it is owned by the client and must
  /// remain valid for the duration of the interpreter lifetime.
  ffi.Pointer<TfLiteModel> TfLiteModelCreateFromFileWithErrorReporter(
    ffi.Pointer<ffi.Char> model_path,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> format, ffi.Pointer<ffi.Char> args)>>
        reporter,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _TfLiteModelCreateFromFileWithErrorReporter(
      model_path,
      reporter,
      user_data,
    );
  }

  late final _TfLiteModelCreateFromFileWithErrorReporterPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteModel> Function(
                  ffi.Pointer<ffi.Char>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<ffi.Char> format,
                              ffi.Pointer<ffi.Char> args)>>,
                  ffi.Pointer<ffi.Void>)>>(
      'TfLiteModelCreateFromFileWithErrorReporter');
  late final _TfLiteModelCreateFromFileWithErrorReporter =
      _TfLiteModelCreateFromFileWithErrorReporterPtr.asFunction<
          ffi.Pointer<TfLiteModel> Function(
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> format,
                          ffi.Pointer<ffi.Char> args)>>,
              ffi.Pointer<ffi.Void>)>();

  /// Destroys the model instance.
  ///
  /// If `model` is a null pointer, this function has no effect.
  void TfLiteModelDelete(
    ffi.Pointer<TfLiteModel> model,
  ) {
    return _TfLiteModelDelete(
      model,
    );
  }

  late final _TfLiteModelDeletePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteModel>)>>(
          'TfLiteModelDelete');
  late final _TfLiteModelDelete = _TfLiteModelDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteModel>)>();

  /// Returns a new interpreter options instances.
  ffi.Pointer<TfLiteInterpreterOptions> TfLiteInterpreterOptionsCreate() {
    return _TfLiteInterpreterOptionsCreate();
  }

  late final _TfLiteInterpreterOptionsCreatePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<TfLiteInterpreterOptions> Function()>>(
      'TfLiteInterpreterOptionsCreate');
  late final _TfLiteInterpreterOptionsCreate =
      _TfLiteInterpreterOptionsCreatePtr.asFunction<
          ffi.Pointer<TfLiteInterpreterOptions> Function()>();

  /// Creates and returns a shallow copy of an options object.
  ///
  /// The caller is responsible for calling `TfLiteInterpreterOptionsDelete` to
  /// deallocate the object pointed to by the returned pointer.
  ffi.Pointer<TfLiteInterpreterOptions> TfLiteInterpreterOptionsCopy(
    ffi.Pointer<TfLiteInterpreterOptions> from,
  ) {
    return _TfLiteInterpreterOptionsCopy(
      from,
    );
  }

  late final _TfLiteInterpreterOptionsCopyPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteInterpreterOptions> Function(
                  ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterOptionsCopy');
  late final _TfLiteInterpreterOptionsCopy =
      _TfLiteInterpreterOptionsCopyPtr.asFunction<
          ffi.Pointer<TfLiteInterpreterOptions> Function(
              ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Destroys the interpreter options instance.
  ///
  /// If `options` is a null pointer, this function has no effect.
  void TfLiteInterpreterOptionsDelete(
    ffi.Pointer<TfLiteInterpreterOptions> options,
  ) {
    return _TfLiteInterpreterOptionsDelete(
      options,
    );
  }

  late final _TfLiteInterpreterOptionsDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterOptionsDelete');
  late final _TfLiteInterpreterOptionsDelete =
      _TfLiteInterpreterOptionsDeletePtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Sets the number of CPU threads to use for the interpreter.
  void TfLiteInterpreterOptionsSetNumThreads(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    int num_threads,
  ) {
    return _TfLiteInterpreterOptionsSetNumThreads(
      options,
      num_threads,
    );
  }

  late final _TfLiteInterpreterOptionsSetNumThreadsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Int32)>>('TfLiteInterpreterOptionsSetNumThreads');
  late final _TfLiteInterpreterOptionsSetNumThreads =
      _TfLiteInterpreterOptionsSetNumThreadsPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>, int)>();

  /// Adds a delegate to be applied during `TfLiteInterpreter` creation.
  ///
  /// If delegate application fails, interpreter creation will also fail with an
  /// associated error logged.
  ///
  /// \note The caller retains ownership of the delegate and should ensure that it
  /// remains valid for the duration of any created interpreter's lifetime.
  ///
  /// If you are NOT using "TensorFlow Lite in Play Services", and NOT building
  /// with `TFLITE_WITH_STABLE_ABI` or `TFLITE_USE_OPAQUE_DELEGATE` macros
  /// enabled, it is possible to pass a `TfLiteDelegate*` rather than a
  /// `TfLiteOpaqueDelegate*` to this function, since in those cases,
  /// `TfLiteOpaqueDelegate` is just a typedef alias for `TfLiteDelegate`.
  /// This is for compatibility with existing source code
  /// and existing delegates.  For new delegates, it is recommended to
  /// use `TfLiteOpaqueDelegate` rather than `TfLiteDelegate`.  (See
  /// `TfLiteOpaqueDelegate` in tensorflow/lite/core/c/c_api_types.h.)
  void TfLiteInterpreterOptionsAddDelegate(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<TfLiteOpaqueDelegate> delegate,
  ) {
    return _TfLiteInterpreterOptionsAddDelegate(
      options,
      delegate,
    );
  }

  late final _TfLiteInterpreterOptionsAddDelegatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<TfLiteOpaqueDelegate>)>>(
      'TfLiteInterpreterOptionsAddDelegate');
  late final _TfLiteInterpreterOptionsAddDelegate =
      _TfLiteInterpreterOptionsAddDelegatePtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<TfLiteOpaqueDelegate>)>();

  /// Sets a custom error reporter for interpreter execution.
  ///
  /// * `reporter` takes the provided `user_data` object, as well as a C-style
  /// format string and arg list (see also vprintf).
  /// * `user_data` is optional. If non-null, it is owned by the client and must
  /// remain valid for the duration of the interpreter lifetime.
  void TfLiteInterpreterOptionsSetErrorReporter(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<
            ffi.NativeFunction<
                ffi.Void Function(ffi.Pointer<ffi.Void> user_data,
                    ffi.Pointer<ffi.Char> format, ffi.Pointer<ffi.Char> args)>>
        reporter,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _TfLiteInterpreterOptionsSetErrorReporter(
      options,
      reporter,
      user_data,
    );
  }

  late final _TfLiteInterpreterOptionsSetErrorReporterPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<
                      ffi.NativeFunction<
                          ffi.Void Function(
                              ffi.Pointer<ffi.Void> user_data,
                              ffi.Pointer<ffi.Char> format,
                              ffi.Pointer<ffi.Char> args)>>,
                  ffi.Pointer<ffi.Void>)>>(
      'TfLiteInterpreterOptionsSetErrorReporter');
  late final _TfLiteInterpreterOptionsSetErrorReporter =
      _TfLiteInterpreterOptionsSetErrorReporterPtr.asFunction<
          void Function(
              ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<
                  ffi.NativeFunction<
                      ffi.Void Function(
                          ffi.Pointer<ffi.Void> user_data,
                          ffi.Pointer<ffi.Char> format,
                          ffi.Pointer<ffi.Char> args)>>,
              ffi.Pointer<ffi.Void>)>();

  /// Adds an op registration to be applied during `TfLiteInterpreter` creation.
  ///
  /// The `TfLiteOperator` object is needed to implement custom op of
  /// TFLite Interpreter via C API. Calling this function ensures that any
  /// `TfLiteInterpreter` created with the specified `options` can execute models
  /// that use the custom operator specified in `registration`.
  /// Please refer https://www.tensorflow.org/lite/guide/ops_custom for custom op
  /// support.
  /// \note The caller retains ownership of the TfLiteOperator object
  /// and should ensure that it remains valid for the duration of any created
  /// interpreter's lifetime.
  /// \warning This is an experimental API and subject to change.
  void TfLiteInterpreterOptionsAddOperator(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    ffi.Pointer<TfLiteOperator> registration,
  ) {
    return _TfLiteInterpreterOptionsAddOperator(
      options,
      registration,
    );
  }

  late final _TfLiteInterpreterOptionsAddOperatorPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteInterpreterOptions>,
                  ffi.Pointer<TfLiteOperator>)>>(
      'TfLiteInterpreterOptionsAddOperator');
  late final _TfLiteInterpreterOptionsAddOperator =
      _TfLiteInterpreterOptionsAddOperatorPtr.asFunction<
          void Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Pointer<TfLiteOperator>)>();

  /// Enables users to cancel in-flight invocations with
  /// `TfLiteInterpreterCancel`.
  ///
  /// By default it is disabled and calling to `TfLiteInterpreterCancel` will
  /// return kTfLiteError. See `TfLiteInterpreterCancel`.
  TfLiteStatus TfLiteInterpreterOptionsEnableCancellation(
    ffi.Pointer<TfLiteInterpreterOptions> options,
    bool enable,
  ) {
    return TfLiteStatus.fromValue(_TfLiteInterpreterOptionsEnableCancellation(
      options,
      enable,
    ));
  }

  late final _TfLiteInterpreterOptionsEnableCancellationPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteInterpreterOptions>,
              ffi.Bool)>>('TfLiteInterpreterOptionsEnableCancellation');
  late final _TfLiteInterpreterOptionsEnableCancellation =
      _TfLiteInterpreterOptionsEnableCancellationPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreterOptions>, bool)>();

  /// Returns a new interpreter using the provided model and options, or null on
  /// failure.
  ///
  /// * `model` must be a valid model instance. The caller retains ownership of
  /// the object, and may destroy it (via TfLiteModelDelete) immediately after
  /// creating the interpreter.  However, if the TfLiteModel was allocated with
  /// TfLiteModelCreate, then the `model_data` buffer that was passed to
  /// TfLiteModelCreate must outlive the lifetime of the TfLiteInterpreter
  /// object that this function returns, and must not be modified during that
  /// time; and if the TfLiteModel was allocated with TfLiteModelCreateFromFile,
  /// then the contents of the model file must not be modified during the
  /// lifetime of the TfLiteInterpreter object that this function returns.
  /// * `optional_options` may be null. The caller retains ownership of the
  /// object, and can safely destroy it (via TfLiteInterpreterOptionsDelete)
  /// immediately after creating the interpreter.
  ///
  /// \note The client *must* explicitly allocate tensors before attempting to
  /// access input tensor data or invoke the interpreter.
  ffi.Pointer<TfLiteInterpreter> TfLiteInterpreterCreate(
    ffi.Pointer<TfLiteModel> model,
    ffi.Pointer<TfLiteInterpreterOptions> optional_options,
  ) {
    return _TfLiteInterpreterCreate(
      model,
      optional_options,
    );
  }

  late final _TfLiteInterpreterCreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteInterpreter> Function(ffi.Pointer<TfLiteModel>,
                  ffi.Pointer<TfLiteInterpreterOptions>)>>(
      'TfLiteInterpreterCreate');
  late final _TfLiteInterpreterCreate = _TfLiteInterpreterCreatePtr.asFunction<
      ffi.Pointer<TfLiteInterpreter> Function(
          ffi.Pointer<TfLiteModel>, ffi.Pointer<TfLiteInterpreterOptions>)>();

  /// Destroys the interpreter.
  ///
  /// If `interpreter` is a null pointer, this function has no effect.
  void TfLiteInterpreterDelete(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterDelete(
      interpreter,
    );
  }

  late final _TfLiteInterpreterDeletePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterDelete');
  late final _TfLiteInterpreterDelete = _TfLiteInterpreterDeletePtr.asFunction<
      void Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the number of input tensors associated with the model.
  int TfLiteInterpreterGetInputTensorCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetInputTensorCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetInputTensorCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetInputTensorCount');
  late final _TfLiteInterpreterGetInputTensorCount =
      _TfLiteInterpreterGetInputTensorCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns a pointer to an array of input tensor indices.  The length of the
  /// array can be obtained via a call to `TfLiteInterpreterGetInputTensorCount`.
  ///
  /// Typically the input tensors associated with an `interpreter` would be set
  /// during the initialization of the `interpreter`, through a mechanism like the
  /// `InterpreterBuilder`, and remain unchanged throughout the lifetime of the
  /// interpreter.  However, there are some circumstances in which the pointer may
  /// not remain valid throughout the lifetime of the interpreter, because calls
  /// to `SetInputs` on the interpreter invalidate the returned pointer.
  ///
  /// The ownership of the array remains with the TFLite runtime.
  ffi.Pointer<ffi.Int> TfLiteInterpreterInputTensorIndices(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterInputTensorIndices(
      interpreter,
    );
  }

  late final _TfLiteInterpreterInputTensorIndicesPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Int> Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterInputTensorIndices');
  late final _TfLiteInterpreterInputTensorIndices =
      _TfLiteInterpreterInputTensorIndicesPtr.asFunction<
          ffi.Pointer<ffi.Int> Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the tensor associated with the input index.
  /// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
  ffi.Pointer<TfLiteTensor> TfLiteInterpreterGetInputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int input_index,
  ) {
    return _TfLiteInterpreterGetInputTensor(
      interpreter,
      input_index,
    );
  }

  late final _TfLiteInterpreterGetInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetInputTensor');
  late final _TfLiteInterpreterGetInputTensor =
      _TfLiteInterpreterGetInputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Resizes the specified input tensor.
  ///
  /// \note After a resize, the client *must* explicitly allocate tensors before
  /// attempting to access the resized tensor data or invoke the interpreter.
  ///
  /// REQUIRES: 0 <= input_index < TfLiteInterpreterGetInputTensorCount(tensor)
  ///
  /// This function makes a copy of the input dimensions, so the client can safely
  /// deallocate `input_dims` immediately after this function returns.
  TfLiteStatus TfLiteInterpreterResizeInputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int input_index,
    ffi.Pointer<ffi.Int> input_dims,
    int input_dims_size,
  ) {
    return TfLiteStatus.fromValue(_TfLiteInterpreterResizeInputTensor(
      interpreter,
      input_index,
      input_dims,
      input_dims_size,
    ));
  }

  late final _TfLiteInterpreterResizeInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32,
              ffi.Pointer<ffi.Int>,
              ffi.Int32)>>('TfLiteInterpreterResizeInputTensor');
  late final _TfLiteInterpreterResizeInputTensor =
      _TfLiteInterpreterResizeInputTensorPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>, int,
              ffi.Pointer<ffi.Int>, int)>();

  /// Updates allocations for all tensors, resizing dependent tensors using the
  /// specified input tensor dimensionality.
  ///
  /// This is a relatively expensive operation, and need only be called after
  /// creating the graph and/or resizing any inputs.
  TfLiteStatus TfLiteInterpreterAllocateTensors(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return TfLiteStatus.fromValue(_TfLiteInterpreterAllocateTensors(
      interpreter,
    ));
  }

  late final _TfLiteInterpreterAllocateTensorsPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterAllocateTensors');
  late final _TfLiteInterpreterAllocateTensors =
      _TfLiteInterpreterAllocateTensorsPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Runs inference for the loaded graph.
  ///
  /// Before calling this function, the caller should first invoke
  /// TfLiteInterpreterAllocateTensors() and should also set the values for the
  /// input tensors.  After successfully calling this function, the values for the
  /// output tensors will be set.
  ///
  /// \note It is possible that the interpreter is not in a ready state to
  /// evaluate (e.g., if AllocateTensors() hasn't been called, or if a
  /// ResizeInputTensor() has been performed without a subsequent call to
  /// AllocateTensors()).
  ///
  /// If the (experimental!) delegate fallback option was enabled in the
  /// interpreter options, then the interpreter will automatically fall back to
  /// not using any delegates if execution with delegates fails. For details,
  /// see TfLiteInterpreterOptionsSetEnableDelegateFallback in
  /// c_api_experimental.h.
  ///
  /// Returns one of the following status codes:
  /// - kTfLiteOk: Success. Output is valid.
  /// - kTfLiteDelegateError: Execution with delegates failed, due to a problem
  /// with the delegate(s). If fallback was not enabled, output is invalid.
  /// If fallback was enabled, this return value indicates that fallback
  /// succeeded, the output is valid, and all delegates previously applied to
  /// the interpreter have been undone.
  /// - kTfLiteApplicationError: Same as for kTfLiteDelegateError, except that
  /// the problem was not with the delegate itself, but rather was
  /// due to an incompatibility between the delegate(s) and the
  /// interpreter or model.
  /// - kTfLiteError: Unexpected/runtime failure. Output is invalid.
  TfLiteStatus TfLiteInterpreterInvoke(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return TfLiteStatus.fromValue(_TfLiteInterpreterInvoke(
      interpreter,
    ));
  }

  late final _TfLiteInterpreterInvokePtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteInterpreter>)>>('TfLiteInterpreterInvoke');
  late final _TfLiteInterpreterInvoke = _TfLiteInterpreterInvokePtr.asFunction<
      int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the number of output tensors associated with the model.
  int TfLiteInterpreterGetOutputTensorCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetOutputTensorCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetOutputTensorCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetOutputTensorCount');
  late final _TfLiteInterpreterGetOutputTensorCount =
      _TfLiteInterpreterGetOutputTensorCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns a pointer to an array of output tensor indices.  The length of the
  /// array can be obtained via a call to `TfLiteInterpreterGetOutputTensorCount`.
  ///
  /// Typically the output tensors associated with an `interpreter` would be set
  /// during the initialization of the `interpreter`, through a mechanism like the
  /// `InterpreterBuilder`, and remain unchanged throughout the lifetime of the
  /// interpreter.  However, there are some circumstances in which the pointer may
  /// not remain valid throughout the lifetime of the interpreter, because calls
  /// to `SetOutputs` on the interpreter invalidate the returned pointer.
  ///
  /// The ownership of the array remains with the TFLite runtime.
  ffi.Pointer<ffi.Int> TfLiteInterpreterOutputTensorIndices(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterOutputTensorIndices(
      interpreter,
    );
  }

  late final _TfLiteInterpreterOutputTensorIndicesPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Int> Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterOutputTensorIndices');
  late final _TfLiteInterpreterOutputTensorIndices =
      _TfLiteInterpreterOutputTensorIndicesPtr.asFunction<
          ffi.Pointer<ffi.Int> Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the tensor associated with the output index.
  /// REQUIRES: 0 <= output_index < TfLiteInterpreterGetOutputTensorCount(tensor)
  ///
  /// \note The shape and underlying data buffer for output tensors may be not
  /// be available until after the output tensor has been both sized and
  /// allocated.
  /// In general, best practice is to interact with the output tensor *after*
  /// calling TfLiteInterpreterInvoke().
  ffi.Pointer<TfLiteTensor> TfLiteInterpreterGetOutputTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int output_index,
  ) {
    return _TfLiteInterpreterGetOutputTensor(
      interpreter,
      output_index,
    );
  }

  late final _TfLiteInterpreterGetOutputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetOutputTensor');
  late final _TfLiteInterpreterGetOutputTensor =
      _TfLiteInterpreterGetOutputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns modifiable access to the tensor that corresponds to the
  /// specified `index` and is associated with the provided `interpreter`.
  ///
  /// This requires the `index` to be between 0 and N - 1, where N is the
  /// number of tensors in the model.
  ///
  /// Typically the tensors associated with the `interpreter` would be set during
  /// the `interpreter` initialization, through a mechanism like the
  /// `InterpreterBuilder`, and remain unchanged throughout the lifetime of the
  /// interpreter.  However, there are some circumstances in which the pointer may
  /// not remain valid throughout the lifetime of the interpreter, because calls
  /// to `AddTensors` on the interpreter invalidate the returned pointer.
  ///
  /// Note the difference between this function and
  /// `TfLiteInterpreterGetInputTensor` (or `TfLiteInterpreterGetOutputTensor` for
  /// that matter): `TfLiteInterpreterGetTensor` takes an index into the array of
  /// all tensors associated with the `interpreter`'s model, whereas
  /// `TfLiteInterpreterGetInputTensor` takes an index into the array of input
  /// tensors.
  ///
  /// The ownership of the tensor remains with the TFLite runtime, meaning the
  /// caller should not deallocate the pointer.
  ffi.Pointer<TfLiteTensor> TfLiteInterpreterGetTensor(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int index,
  ) {
    return _TfLiteInterpreterGetTensor(
      interpreter,
      index,
    );
  }

  late final _TfLiteInterpreterGetTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int)>>('TfLiteInterpreterGetTensor');
  late final _TfLiteInterpreterGetTensor =
      _TfLiteInterpreterGetTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Tries to cancel any in-flight invocation.
  ///
  /// \note This only cancels `TfLiteInterpreterInvoke` calls that happen before
  /// calling this and it does not cancel subsequent invocations.
  /// \note Calling this function will also cancel any in-flight invocations of
  /// SignatureRunners constructed from this interpreter.
  /// Non-blocking and thread safe.
  ///
  /// Returns kTfLiteError if cancellation is not enabled via
  /// `TfLiteInterpreterOptionsEnableCancellation`.
  TfLiteStatus TfLiteInterpreterCancel(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return TfLiteStatus.fromValue(_TfLiteInterpreterCancel(
      interpreter,
    ));
  }

  late final _TfLiteInterpreterCancelPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteInterpreter>)>>('TfLiteInterpreterCancel');
  late final _TfLiteInterpreterCancel = _TfLiteInterpreterCancelPtr.asFunction<
      int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the number of signatures defined in the model.
  int TfLiteInterpreterGetSignatureCount(
    ffi.Pointer<TfLiteInterpreter> interpreter,
  ) {
    return _TfLiteInterpreterGetSignatureCount(
      interpreter,
    );
  }

  late final _TfLiteInterpreterGetSignatureCountPtr = _lookup<
          ffi
          .NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteInterpreter>)>>(
      'TfLiteInterpreterGetSignatureCount');
  late final _TfLiteInterpreterGetSignatureCount =
      _TfLiteInterpreterGetSignatureCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteInterpreter>)>();

  /// Returns the key of the Nth signature in the model, where N is specified as
  /// `signature_index`.
  ///
  /// NOTE: The lifetime of the returned key is the same as (and depends on) the
  /// lifetime of `interpreter`.
  ffi.Pointer<ffi.Char> TfLiteInterpreterGetSignatureKey(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    int signature_index,
  ) {
    return _TfLiteInterpreterGetSignatureKey(
      interpreter,
      signature_index,
    );
  }

  late final _TfLiteInterpreterGetSignatureKeyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteInterpreter>,
              ffi.Int32)>>('TfLiteInterpreterGetSignatureKey');
  late final _TfLiteInterpreterGetSignatureKey =
      _TfLiteInterpreterGetSignatureKeyPtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteInterpreter>, int)>();

  /// Returns a new signature runner using the provided interpreter and signature
  /// key, or nullptr on failure.
  ///
  /// NOTE: `signature_key` is a null-terminated C string that must match the
  /// key of a signature in the interpreter's model.
  ///
  /// NOTE: The returned signature runner should be destroyed, by calling
  /// TfLiteSignatureRunnerDelete(), before the interpreter is destroyed.
  ffi.Pointer<TfLiteSignatureRunner> TfLiteInterpreterGetSignatureRunner(
    ffi.Pointer<TfLiteInterpreter> interpreter,
    ffi.Pointer<ffi.Char> signature_key,
  ) {
    return _TfLiteInterpreterGetSignatureRunner(
      interpreter,
      signature_key,
    );
  }

  late final _TfLiteInterpreterGetSignatureRunnerPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteSignatureRunner> Function(
              ffi.Pointer<TfLiteInterpreter>,
              ffi.Pointer<ffi.Char>)>>('TfLiteInterpreterGetSignatureRunner');
  late final _TfLiteInterpreterGetSignatureRunner =
      _TfLiteInterpreterGetSignatureRunnerPtr.asFunction<
          ffi.Pointer<TfLiteSignatureRunner> Function(
              ffi.Pointer<TfLiteInterpreter>, ffi.Pointer<ffi.Char>)>();

  /// Returns the number of inputs associated with a signature.
  int TfLiteSignatureRunnerGetInputCount(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerGetInputCount(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerGetInputCountPtr = _lookup<
          ffi.NativeFunction<
              ffi.Size Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerGetInputCount');
  late final _TfLiteSignatureRunnerGetInputCount =
      _TfLiteSignatureRunnerGetInputCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the (null-terminated) name of the Nth input in a signature, where N
  /// is specified as `input_index`.
  ///
  /// NOTE: The lifetime of the returned name is the same as (and depends on) the
  /// lifetime of `signature_runner`.
  ffi.Pointer<ffi.Char> TfLiteSignatureRunnerGetInputName(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    int input_index,
  ) {
    return _TfLiteSignatureRunnerGetInputName(
      signature_runner,
      input_index,
    );
  }

  late final _TfLiteSignatureRunnerGetInputNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Int32)>>('TfLiteSignatureRunnerGetInputName');
  late final _TfLiteSignatureRunnerGetInputName =
      _TfLiteSignatureRunnerGetInputNamePtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteSignatureRunner>, int)>();

  /// Resizes the input tensor identified as `input_name` to be the dimensions
  /// specified by `input_dims` and `input_dims_size`. Only unknown dimensions can
  /// be resized with this function. Unknown dimensions are indicated as `-1` in
  /// the `dims_signature` attribute of a TfLiteTensor.
  ///
  /// Returns status of failure or success. Note that this doesn't actually resize
  /// any existing buffers. A call to TfLiteSignatureRunnerAllocateTensors() is
  /// required to change the tensor input buffer.
  ///
  /// NOTE: This function is similar to TfLiteInterpreterResizeInputTensorStrict()
  /// and not TfLiteInterpreterResizeInputTensor().
  ///
  /// NOTE: `input_name` must match the name of an input in the signature.
  ///
  /// NOTE: This function makes a copy of the input dimensions, so the caller can
  /// safely deallocate `input_dims` immediately after this function returns.
  TfLiteStatus TfLiteSignatureRunnerResizeInputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> input_name,
    ffi.Pointer<ffi.Int> input_dims,
    int input_dims_size,
  ) {
    return TfLiteStatus.fromValue(_TfLiteSignatureRunnerResizeInputTensor(
      signature_runner,
      input_name,
      input_dims,
      input_dims_size,
    ));
  }

  late final _TfLiteSignatureRunnerResizeInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Int>,
              ffi.Int32)>>('TfLiteSignatureRunnerResizeInputTensor');
  late final _TfLiteSignatureRunnerResizeInputTensor =
      _TfLiteSignatureRunnerResizeInputTensorPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Int>, int)>();

  /// Updates allocations for tensors associated with a signature and resizes
  /// dependent tensors using the specified input tensor dimensionality.
  /// This is a relatively expensive operation and hence should only be called
  /// after initializing the signature runner object and/or resizing any inputs.
  TfLiteStatus TfLiteSignatureRunnerAllocateTensors(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return TfLiteStatus.fromValue(_TfLiteSignatureRunnerAllocateTensors(
      signature_runner,
    ));
  }

  late final _TfLiteSignatureRunnerAllocateTensorsPtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerAllocateTensors');
  late final _TfLiteSignatureRunnerAllocateTensors =
      _TfLiteSignatureRunnerAllocateTensorsPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the input tensor identified by `input_name` in the given signature.
  /// Returns nullptr if the given name is not valid.
  ///
  /// NOTE: The lifetime of the returned tensor is the same as (and depends on)
  /// the lifetime of `signature_runner`.
  ffi.Pointer<TfLiteTensor> TfLiteSignatureRunnerGetInputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> input_name,
  ) {
    return _TfLiteSignatureRunnerGetInputTensor(
      signature_runner,
      input_name,
    );
  }

  late final _TfLiteSignatureRunnerGetInputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>)>>('TfLiteSignatureRunnerGetInputTensor');
  late final _TfLiteSignatureRunnerGetInputTensor =
      _TfLiteSignatureRunnerGetInputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteSignatureRunner>, ffi.Pointer<ffi.Char>)>();

  /// Runs inference on a given signature.
  ///
  /// Before calling this function, the caller should first invoke
  /// TfLiteSignatureRunnerAllocateTensors() and should also set the values for
  /// the input tensors. After successfully calling this function, the values for
  /// the output tensors will be set.
  TfLiteStatus TfLiteSignatureRunnerInvoke(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return TfLiteStatus.fromValue(_TfLiteSignatureRunnerInvoke(
      signature_runner,
    ));
  }

  late final _TfLiteSignatureRunnerInvokePtr = _lookup<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerInvoke');
  late final _TfLiteSignatureRunnerInvoke = _TfLiteSignatureRunnerInvokePtr
      .asFunction<int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the number of output tensors associated with the signature.
  int TfLiteSignatureRunnerGetOutputCount(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerGetOutputCount(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputCountPtr = _lookup<
          ffi.NativeFunction<
              ffi.Size Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerGetOutputCount');
  late final _TfLiteSignatureRunnerGetOutputCount =
      _TfLiteSignatureRunnerGetOutputCountPtr.asFunction<
          int Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Returns the (null-terminated) name of the Nth output in a signature, where
  /// N is specified as `output_index`.
  ///
  /// NOTE: The lifetime of the returned name is the same as (and depends on) the
  /// lifetime of `signature_runner`.
  ffi.Pointer<ffi.Char> TfLiteSignatureRunnerGetOutputName(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    int output_index,
  ) {
    return _TfLiteSignatureRunnerGetOutputName(
      signature_runner,
      output_index,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Int32)>>('TfLiteSignatureRunnerGetOutputName');
  late final _TfLiteSignatureRunnerGetOutputName =
      _TfLiteSignatureRunnerGetOutputNamePtr.asFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteSignatureRunner>, int)>();

  /// Returns the output tensor identified by `output_name` in the given
  /// signature. Returns nullptr if the given name is not valid.
  ///
  /// NOTE: The lifetime of the returned tensor is the same as (and depends on)
  /// the lifetime of `signature_runner`.
  ffi.Pointer<TfLiteTensor> TfLiteSignatureRunnerGetOutputTensor(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
    ffi.Pointer<ffi.Char> output_name,
  ) {
    return _TfLiteSignatureRunnerGetOutputTensor(
      signature_runner,
      output_name,
    );
  }

  late final _TfLiteSignatureRunnerGetOutputTensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteTensor> Function(ffi.Pointer<TfLiteSignatureRunner>,
              ffi.Pointer<ffi.Char>)>>('TfLiteSignatureRunnerGetOutputTensor');
  late final _TfLiteSignatureRunnerGetOutputTensor =
      _TfLiteSignatureRunnerGetOutputTensorPtr.asFunction<
          ffi.Pointer<TfLiteTensor> Function(
              ffi.Pointer<TfLiteSignatureRunner>, ffi.Pointer<ffi.Char>)>();

  /// Returns the type of a tensor element.
  TfLiteType TfLiteTensorType(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return TfLiteType.fromValue(_TfLiteTensorType(
      tensor,
    ));
  }

  late final _TfLiteTensorTypePtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorType');
  late final _TfLiteTensorType = _TfLiteTensorTypePtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the number of dimensions that the tensor has.  Returns -1 in case
  /// the 'opaque_tensor' does not have its dimensions property set.
  int TfLiteTensorNumDims(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorNumDims(
      tensor,
    );
  }

  late final _TfLiteTensorNumDimsPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorNumDims');
  late final _TfLiteTensorNumDims = _TfLiteTensorNumDimsPtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the length of the tensor in the "dim_index" dimension.
  /// REQUIRES: 0 <= dim_index < TFLiteTensorNumDims(tensor)
  int TfLiteTensorDim(
    ffi.Pointer<TfLiteTensor> tensor,
    int dim_index,
  ) {
    return _TfLiteTensorDim(
      tensor,
      dim_index,
    );
  }

  late final _TfLiteTensorDimPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<TfLiteTensor>, ffi.Int32)>>('TfLiteTensorDim');
  late final _TfLiteTensorDim = _TfLiteTensorDimPtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>, int)>();

  /// Returns the size of the underlying data in bytes.
  int TfLiteTensorByteSize(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorByteSize(
      tensor,
    );
  }

  late final _TfLiteTensorByteSizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<TfLiteTensor>)>>(
          'TfLiteTensorByteSize');
  late final _TfLiteTensorByteSize = _TfLiteTensorByteSizePtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns a pointer to the underlying data buffer.
  ///
  /// \note The result may be null if tensors have not yet been allocated, e.g.,
  /// if the Tensor has just been created or resized and `TfLiteAllocateTensors()`
  /// has yet to be called, or if the output tensor is dynamically sized and the
  /// interpreter hasn't been invoked.
  ffi.Pointer<ffi.Void> TfLiteTensorData(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorData(
      tensor,
    );
  }

  late final _TfLiteTensorDataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorData');
  late final _TfLiteTensorData = _TfLiteTensorDataPtr.asFunction<
      ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the (null-terminated) name of the tensor.
  ffi.Pointer<ffi.Char> TfLiteTensorName(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorName(
      tensor,
    );
  }

  late final _TfLiteTensorNamePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorName');
  late final _TfLiteTensorName = _TfLiteTensorNamePtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the parameters for asymmetric quantization. The quantization
  /// parameters are only valid when the tensor type is `kTfLiteUInt8` and the
  /// `scale != 0`. Quantized values can be converted back to float using:
  /// real_value = scale * (quantized_value - zero_point);
  TfLiteQuantizationParams TfLiteTensorQuantizationParams(
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorQuantizationParams(
      tensor,
    );
  }

  late final _TfLiteTensorQuantizationParamsPtr = _lookup<
      ffi.NativeFunction<
          TfLiteQuantizationParams Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorQuantizationParams');
  late final _TfLiteTensorQuantizationParams =
      _TfLiteTensorQuantizationParamsPtr.asFunction<
          TfLiteQuantizationParams Function(ffi.Pointer<TfLiteTensor>)>();

  /// Copies from the provided input buffer into the tensor's buffer.
  /// REQUIRES: input_data_size == TfLiteTensorByteSize(tensor)
  TfLiteStatus TfLiteTensorCopyFromBuffer(
    ffi.Pointer<TfLiteTensor> tensor,
    ffi.Pointer<ffi.Void> input_data,
    int input_data_size,
  ) {
    return TfLiteStatus.fromValue(_TfLiteTensorCopyFromBuffer(
      tensor,
      input_data,
      input_data_size,
    ));
  }

  late final _TfLiteTensorCopyFromBufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>,
              ffi.Pointer<ffi.Void>, ffi.Size)>>('TfLiteTensorCopyFromBuffer');
  late final _TfLiteTensorCopyFromBuffer =
      _TfLiteTensorCopyFromBufferPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>, int)>();

  /// Copies to the provided output buffer from the tensor's buffer.
  /// REQUIRES: output_data_size == TfLiteTensorByteSize(tensor)
  TfLiteStatus TfLiteTensorCopyToBuffer(
    ffi.Pointer<TfLiteTensor> output_tensor,
    ffi.Pointer<ffi.Void> output_data,
    int output_data_size,
  ) {
    return TfLiteStatus.fromValue(_TfLiteTensorCopyToBuffer(
      output_tensor,
      output_data,
      output_data_size,
    ));
  }

  late final _TfLiteTensorCopyToBufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>,
              ffi.Pointer<ffi.Void>, ffi.Size)>>('TfLiteTensorCopyToBuffer');
  late final _TfLiteTensorCopyToBuffer =
      _TfLiteTensorCopyToBufferPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteTensor>, ffi.Pointer<ffi.Void>, int)>();

  /// Destroys the signature runner.
  ///
  /// If `signature_runner` is a null pointer, this function has no effect.
  void TfLiteSignatureRunnerDelete(
    ffi.Pointer<TfLiteSignatureRunner> signature_runner,
  ) {
    return _TfLiteSignatureRunnerDelete(
      signature_runner,
    );
  }

  late final _TfLiteSignatureRunnerDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteSignatureRunner>)>>(
      'TfLiteSignatureRunnerDelete');
  late final _TfLiteSignatureRunnerDelete = _TfLiteSignatureRunnerDeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteSignatureRunner>)>();

  /// Given the size (number of elements) in a TfLiteIntArray, calculate its size
  /// in bytes.
  int TfLiteIntArrayGetSizeInBytes(
    int size,
  ) {
    return _TfLiteIntArrayGetSizeInBytes(
      size,
    );
  }

  late final _TfLiteIntArrayGetSizeInBytesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Int)>>(
          'TfLiteIntArrayGetSizeInBytes');
  late final _TfLiteIntArrayGetSizeInBytes =
      _TfLiteIntArrayGetSizeInBytesPtr.asFunction<int Function(int)>();

  /// Create a array of a given `size` (uninitialized entries).
  /// This returns a pointer, that you must free using TfLiteIntArrayFree().
  ffi.Pointer<TfLiteIntArray> TfLiteIntArrayCreate(
    int size,
  ) {
    return _TfLiteIntArrayCreate(
      size,
    );
  }

  late final _TfLiteIntArrayCreatePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<TfLiteIntArray> Function(ffi.Int)>>(
      'TfLiteIntArrayCreate');
  late final _TfLiteIntArrayCreate = _TfLiteIntArrayCreatePtr.asFunction<
      ffi.Pointer<TfLiteIntArray> Function(int)>();

  /// Check if two intarrays are equal. Returns 1 if they are equal, 0 otherwise.
  int TfLiteIntArrayEqual(
    ffi.Pointer<TfLiteIntArray> a,
    ffi.Pointer<TfLiteIntArray> b,
  ) {
    return _TfLiteIntArrayEqual(
      a,
      b,
    );
  }

  late final _TfLiteIntArrayEqualPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<TfLiteIntArray>,
              ffi.Pointer<TfLiteIntArray>)>>('TfLiteIntArrayEqual');
  late final _TfLiteIntArrayEqual = _TfLiteIntArrayEqualPtr.asFunction<
      int Function(ffi.Pointer<TfLiteIntArray>, ffi.Pointer<TfLiteIntArray>)>();

  /// Check if an intarray equals an array. Returns 1 if equals, 0 otherwise.
  int TfLiteIntArrayEqualsArray(
    ffi.Pointer<TfLiteIntArray> a,
    int b_size,
    ffi.Pointer<ffi.Int> b_data,
  ) {
    return _TfLiteIntArrayEqualsArray(
      a,
      b_size,
      b_data,
    );
  }

  late final _TfLiteIntArrayEqualsArrayPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<TfLiteIntArray>, ffi.Int,
              ffi.Pointer<ffi.Int>)>>('TfLiteIntArrayEqualsArray');
  late final _TfLiteIntArrayEqualsArray =
      _TfLiteIntArrayEqualsArrayPtr.asFunction<
          int Function(
              ffi.Pointer<TfLiteIntArray>, int, ffi.Pointer<ffi.Int>)>();

  /// Create a copy of an array passed as `src`.
  /// You are expected to free memory with TfLiteIntArrayFree
  ffi.Pointer<TfLiteIntArray> TfLiteIntArrayCopy(
    ffi.Pointer<TfLiteIntArray> src,
  ) {
    return _TfLiteIntArrayCopy(
      src,
    );
  }

  late final _TfLiteIntArrayCopyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteIntArray> Function(
              ffi.Pointer<TfLiteIntArray>)>>('TfLiteIntArrayCopy');
  late final _TfLiteIntArrayCopy = _TfLiteIntArrayCopyPtr.asFunction<
      ffi.Pointer<TfLiteIntArray> Function(ffi.Pointer<TfLiteIntArray>)>();

  /// Free memory of array `a`.
  void TfLiteIntArrayFree(
    ffi.Pointer<TfLiteIntArray> a,
  ) {
    return _TfLiteIntArrayFree(
      a,
    );
  }

  late final _TfLiteIntArrayFreePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteIntArray>)>>(
      'TfLiteIntArrayFree');
  late final _TfLiteIntArrayFree = _TfLiteIntArrayFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteIntArray>)>();

  /// Given the size (number of elements) in a TfLiteFloatArray, calculate its
  /// size in bytes.
  int TfLiteFloatArrayGetSizeInBytes(
    int size,
  ) {
    return _TfLiteFloatArrayGetSizeInBytes(
      size,
    );
  }

  late final _TfLiteFloatArrayGetSizeInBytesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Int)>>(
          'TfLiteFloatArrayGetSizeInBytes');
  late final _TfLiteFloatArrayGetSizeInBytes =
      _TfLiteFloatArrayGetSizeInBytesPtr.asFunction<int Function(int)>();

  /// Create a array of a given `size` (uninitialized entries).
  /// This returns a pointer, that you must free using TfLiteFloatArrayFree().
  ffi.Pointer<TfLiteFloatArray> TfLiteFloatArrayCreate(
    int size,
  ) {
    return _TfLiteFloatArrayCreate(
      size,
    );
  }

  late final _TfLiteFloatArrayCreatePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<TfLiteFloatArray> Function(ffi.Int)>>(
      'TfLiteFloatArrayCreate');
  late final _TfLiteFloatArrayCreate = _TfLiteFloatArrayCreatePtr.asFunction<
      ffi.Pointer<TfLiteFloatArray> Function(int)>();

  /// Create a copy of an array passed as `src`.
  /// You are expected to free memory with TfLiteFloatArrayFree.
  ffi.Pointer<TfLiteFloatArray> TfLiteFloatArrayCopy(
    ffi.Pointer<TfLiteFloatArray> src,
  ) {
    return _TfLiteFloatArrayCopy(
      src,
    );
  }

  late final _TfLiteFloatArrayCopyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteFloatArray> Function(
              ffi.Pointer<TfLiteFloatArray>)>>('TfLiteFloatArrayCopy');
  late final _TfLiteFloatArrayCopy = _TfLiteFloatArrayCopyPtr.asFunction<
      ffi.Pointer<TfLiteFloatArray> Function(ffi.Pointer<TfLiteFloatArray>)>();

  /// Free memory of array `a`.
  void TfLiteFloatArrayFree(
    ffi.Pointer<TfLiteFloatArray> a,
  ) {
    return _TfLiteFloatArrayFree(
      a,
    );
  }

  late final _TfLiteFloatArrayFreePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteFloatArray>)>>(
      'TfLiteFloatArrayFree');
  late final _TfLiteFloatArrayFree = _TfLiteFloatArrayFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteFloatArray>)>();

  /// Return the name of a given type, for error reporting purposes.
  ffi.Pointer<ffi.Char> TfLiteTypeGetName(
    TfLiteType type,
  ) {
    return _TfLiteTypeGetName(
      type.value,
    );
  }

  late final _TfLiteTypeGetNamePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>>(
      'TfLiteTypeGetName');
  late final _TfLiteTypeGetName =
      _TfLiteTypeGetNamePtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  /// Free data memory of tensor `t`.
  void TfLiteTensorDataFree(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return _TfLiteTensorDataFree(
      t,
    );
  }

  late final _TfLiteTensorDataFreePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteTensor>)>>(
          'TfLiteTensorDataFree');
  late final _TfLiteTensorDataFree = _TfLiteTensorDataFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteTensor>)>();

  /// Free quantization data.
  void TfLiteQuantizationFree(
    ffi.Pointer<TfLiteQuantization> quantization,
  ) {
    return _TfLiteQuantizationFree(
      quantization,
    );
  }

  late final _TfLiteQuantizationFreePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteQuantization>)>>(
      'TfLiteQuantizationFree');
  late final _TfLiteQuantizationFree = _TfLiteQuantizationFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteQuantization>)>();

  /// Free sparsity parameters.
  void TfLiteSparsityFree(
    ffi.Pointer<TfLiteSparsity> sparsity,
  ) {
    return _TfLiteSparsityFree(
      sparsity,
    );
  }

  late final _TfLiteSparsityFreePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteSparsity>)>>(
      'TfLiteSparsityFree');
  late final _TfLiteSparsityFree = _TfLiteSparsityFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteSparsity>)>();

  /// Free memory of tensor `t`.
  void TfLiteTensorFree(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return _TfLiteTensorFree(
      t,
    );
  }

  late final _TfLiteTensorFreePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<TfLiteTensor>)>>(
          'TfLiteTensorFree');
  late final _TfLiteTensorFree = _TfLiteTensorFreePtr.asFunction<
      void Function(ffi.Pointer<TfLiteTensor>)>();

  /// Set all of a tensor's fields (and free any previously allocated data).
  void TfLiteTensorReset(
    TfLiteType type,
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<TfLiteIntArray> dims,
    TfLiteQuantizationParams quantization,
    ffi.Pointer<ffi.Char> buffer,
    int size,
    TfLiteAllocationType allocation_type,
    ffi.Pointer<ffi.Void> allocation,
    bool is_variable,
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return _TfLiteTensorReset(
      type.value,
      name,
      dims,
      quantization,
      buffer,
      size,
      allocation_type.value,
      allocation,
      is_variable,
      tensor,
    );
  }

  late final _TfLiteTensorResetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.UnsignedInt,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<TfLiteIntArray>,
              TfLiteQuantizationParams,
              ffi.Pointer<ffi.Char>,
              ffi.Size,
              ffi.UnsignedInt,
              ffi.Pointer<ffi.Void>,
              ffi.Bool,
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorReset');
  late final _TfLiteTensorReset = _TfLiteTensorResetPtr.asFunction<
      void Function(
          int,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<TfLiteIntArray>,
          TfLiteQuantizationParams,
          ffi.Pointer<ffi.Char>,
          int,
          int,
          ffi.Pointer<ffi.Void>,
          bool,
          ffi.Pointer<TfLiteTensor>)>();

  /// Copies the contents of `src` in `dst`.
  /// Function does nothing if either `src` or `dst` is passed as nullptr and
  /// return `kTfLiteOk`.
  /// Returns `kTfLiteError` if `src` and `dst` doesn't have matching data size.
  /// Note function copies contents, so it won't create new data pointer
  /// or change allocation type.
  /// All Tensor related properties will be copied from `src` to `dst` like
  /// quantization, sparsity, ...
  TfLiteStatus TfLiteTensorCopy(
    ffi.Pointer<TfLiteTensor> src,
    ffi.Pointer<TfLiteTensor> dst,
  ) {
    return TfLiteStatus.fromValue(_TfLiteTensorCopy(
      src,
      dst,
    ));
  }

  late final _TfLiteTensorCopyPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>,
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorCopy');
  late final _TfLiteTensorCopy = _TfLiteTensorCopyPtr.asFunction<
      int Function(ffi.Pointer<TfLiteTensor>, ffi.Pointer<TfLiteTensor>)>();

  /// Returns a tensor holding a deep copy of src.
  TfLiteTensor TfLiteTensorClone(
    TfLiteTensor src,
  ) {
    return _TfLiteTensorClone(
      src,
    );
  }

  late final _TfLiteTensorClonePtr =
      _lookup<ffi.NativeFunction<TfLiteTensor Function(TfLiteTensor)>>(
          'TfLiteTensorClone');
  late final _TfLiteTensorClone =
      _TfLiteTensorClonePtr.asFunction<TfLiteTensor Function(TfLiteTensor)>();

  /// Change the size of the memory block owned by `tensor` to `num_bytes`.
  /// Tensors with allocation types other than `kTfLiteDynamic` will be ignored
  /// and a `kTfLiteOk` will be returned. `tensor`'s internal data buffer will be
  /// assigned a pointer which can safely be passed to free or realloc if
  /// `num_bytes` is zero. If `preserve_data` is true, tensor data will be
  /// unchanged in the range from the start of the region up to the minimum of the
  /// old and new sizes. In the case of NULL tensor, or an error allocating new
  /// memory, returns `kTfLiteError`.
  TfLiteStatus TfLiteTensorResizeMaybeCopy(
    int num_bytes,
    ffi.Pointer<TfLiteTensor> tensor,
    bool preserve_data,
  ) {
    return TfLiteStatus.fromValue(_TfLiteTensorResizeMaybeCopy(
      num_bytes,
      tensor,
      preserve_data,
    ));
  }

  late final _TfLiteTensorResizeMaybeCopyPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Size, ffi.Pointer<TfLiteTensor>,
              ffi.Bool)>>('TfLiteTensorResizeMaybeCopy');
  late final _TfLiteTensorResizeMaybeCopy = _TfLiteTensorResizeMaybeCopyPtr
      .asFunction<int Function(int, ffi.Pointer<TfLiteTensor>, bool)>();

  /// Change the size of the memory block owned by `tensor` to `num_bytes`.
  /// Tensors with allocation types other than `kTfLiteDynamic` will be ignored
  /// and a `kTfLiteOk` will be returned. `tensor`'s internal data buffer will be
  /// assigned a pointer which can safely be passed to free or realloc if
  /// `num_bytes` is zero. Tensor data will be unchanged in the range from the
  /// start of the region up to the minimum of the old and new sizes. In the case
  /// of NULL tensor, or an error allocating new memory, returns `kTfLiteError`.
  TfLiteStatus TfLiteTensorRealloc(
    int num_bytes,
    ffi.Pointer<TfLiteTensor> tensor,
  ) {
    return TfLiteStatus.fromValue(_TfLiteTensorRealloc(
      num_bytes,
      tensor,
    ));
  }

  late final _TfLiteTensorReallocPtr = _lookup<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Size, ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorRealloc');
  late final _TfLiteTensorRealloc = _TfLiteTensorReallocPtr.asFunction<
      int Function(int, ffi.Pointer<TfLiteTensor>)>();

  /// Returns the shape of the tensor, with -1 for any unknown dimension sizes.
  /// If any dimension is unknown, this is the same as `t->dims_signature`.
  /// If all dimensions are known, this is the same as `t->dims`.
  /// (`dims_signature` is NULL or empty if all dimensions are known.)
  ffi.Pointer<TfLiteIntArray> TfLiteTensorGetDimsSignature(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return _TfLiteTensorGetDimsSignature(
      t,
    );
  }

  late final _TfLiteTensorGetDimsSignaturePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteIntArray> Function(
              ffi.Pointer<TfLiteTensor>)>>('TfLiteTensorGetDimsSignature');
  late final _TfLiteTensorGetDimsSignature =
      _TfLiteTensorGetDimsSignaturePtr.asFunction<
          ffi.Pointer<TfLiteIntArray> Function(ffi.Pointer<TfLiteTensor>)>();

  /// Build a `null` delegate, with all the fields properly set to their default
  /// values.
  TfLiteDelegate TfLiteDelegateCreate() {
    return _TfLiteDelegateCreate();
  }

  late final _TfLiteDelegateCreatePtr =
      _lookup<ffi.NativeFunction<TfLiteDelegate Function()>>(
          'TfLiteDelegateCreate');
  late final _TfLiteDelegateCreate =
      _TfLiteDelegateCreatePtr.asFunction<TfLiteDelegate Function()>();

  /// See c_api_opaque.h.
  /// This declaration in common.h is only for backwards compatibility.
  /// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
  ffi.Pointer<TfLiteOpaqueDelegate> TfLiteOpaqueDelegateCreate(
    ffi.Pointer<TfLiteOpaqueDelegateBuilder> opaque_delegate_builder,
  ) {
    return _TfLiteOpaqueDelegateCreate(
      opaque_delegate_builder,
    );
  }

  late final _TfLiteOpaqueDelegateCreatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteOpaqueDelegate> Function(
                  ffi.Pointer<TfLiteOpaqueDelegateBuilder>)>>(
      'TfLiteOpaqueDelegateCreate');
  late final _TfLiteOpaqueDelegateCreate =
      _TfLiteOpaqueDelegateCreatePtr.asFunction<
          ffi.Pointer<TfLiteOpaqueDelegate> Function(
              ffi.Pointer<TfLiteOpaqueDelegateBuilder>)>();

  /// See c_api_opaque.h.
  /// This declaration in common.h is only for backwards compatibility.
  /// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
  void TfLiteOpaqueDelegateDelete(
    ffi.Pointer<TfLiteOpaqueDelegate> delegate,
  ) {
    return _TfLiteOpaqueDelegateDelete(
      delegate,
    );
  }

  late final _TfLiteOpaqueDelegateDeletePtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<TfLiteOpaqueDelegate>)>>(
      'TfLiteOpaqueDelegateDelete');
  late final _TfLiteOpaqueDelegateDelete = _TfLiteOpaqueDelegateDeletePtr
      .asFunction<void Function(ffi.Pointer<TfLiteOpaqueDelegate>)>();

  /// See c_api_opaque.h.
  /// This declaration in common.h is only for backwards compatibility.
  /// NOTE: This function is part of the TensorFlow Lite Extension APIs, see above.
  ffi.Pointer<ffi.Void> TfLiteOpaqueDelegateGetData(
    ffi.Pointer<TfLiteOpaqueDelegate> delegate,
  ) {
    return _TfLiteOpaqueDelegateGetData(
      delegate,
    );
  }

  late final _TfLiteOpaqueDelegateGetDataPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(
                  ffi.Pointer<TfLiteOpaqueDelegate>)>>(
      'TfLiteOpaqueDelegateGetData');
  late final _TfLiteOpaqueDelegateGetData =
      _TfLiteOpaqueDelegateGetDataPtr.asFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteOpaqueDelegate>)>();

  /// Returns a tensor data allocation strategy.
  TfLiteAllocationStrategy TfLiteTensorGetAllocationStrategy(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return TfLiteAllocationStrategy.fromValue(
        _TfLiteTensorGetAllocationStrategy(
      t,
    ));
  }

  late final _TfLiteTensorGetAllocationStrategyPtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorGetAllocationStrategy');
  late final _TfLiteTensorGetAllocationStrategy =
      _TfLiteTensorGetAllocationStrategyPtr.asFunction<
          int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns how stable a tensor data buffer address is across runs.
  TfLiteRunStability TfLiteTensorGetBufferAddressStability(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return TfLiteRunStability.fromValue(_TfLiteTensorGetBufferAddressStability(
      t,
    ));
  }

  late final _TfLiteTensorGetBufferAddressStabilityPtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorGetBufferAddressStability');
  late final _TfLiteTensorGetBufferAddressStability =
      _TfLiteTensorGetBufferAddressStabilityPtr.asFunction<
          int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns how stable a tensor data values are across runs.
  TfLiteRunStability TfLiteTensorGetDataStability(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return TfLiteRunStability.fromValue(_TfLiteTensorGetDataStability(
      t,
    ));
  }

  late final _TfLiteTensorGetDataStabilityPtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorGetDataStability');
  late final _TfLiteTensorGetDataStability = _TfLiteTensorGetDataStabilityPtr
      .asFunction<int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the operation step when the data of a tensor is populated.
  ///
  /// Some operations can precompute their results before the evaluation step.
  /// This makes the data available earlier for subsequent operations.
  TfLiteRunStep TfLiteTensorGetDataKnownStep(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return TfLiteRunStep.fromValue(_TfLiteTensorGetDataKnownStep(
      t,
    ));
  }

  late final _TfLiteTensorGetDataKnownStepPtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorGetDataKnownStep');
  late final _TfLiteTensorGetDataKnownStep = _TfLiteTensorGetDataKnownStepPtr
      .asFunction<int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns the operation steop when the shape of a tensor is computed.
  ///
  /// Some operations can precompute the shape of their results before the
  /// evaluation step. This makes the shape available earlier for subsequent
  /// operations.
  TfLiteRunStep TfLiteTensorGetShapeKnownStep(
    ffi.Pointer<TfLiteTensor> t,
  ) {
    return TfLiteRunStep.fromValue(_TfLiteTensorGetShapeKnownStep(
      t,
    ));
  }

  late final _TfLiteTensorGetShapeKnownStepPtr = _lookup<
          ffi
          .NativeFunction<ffi.UnsignedInt Function(ffi.Pointer<TfLiteTensor>)>>(
      'TfLiteTensorGetShapeKnownStep');
  late final _TfLiteTensorGetShapeKnownStep = _TfLiteTensorGetShapeKnownStepPtr
      .asFunction<int Function(ffi.Pointer<TfLiteTensor>)>();

  /// Returns a sentinel value to be used as the user_data field of a TfLiteNode
  /// when the kernel initialization fails.
  ffi.Pointer<ffi.Void> TfLiteKernelInitFailed() {
    return _TfLiteKernelInitFailed();
  }

  late final _TfLiteKernelInitFailedPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Void> Function()>>(
          'TfLiteKernelInitFailed');
  late final _TfLiteKernelInitFailed =
      _TfLiteKernelInitFailedPtr.asFunction<ffi.Pointer<ffi.Void> Function()>();
}

/// The enum for builtin operators.
/// Note: CUSTOM, DELEGATE, and PLACEHOLDER_FOR_GREATER_OP_CODES are 3 special
/// ops which are not real built-in ops.
enum TfLiteBuiltinOperator {
  kTfLiteBuiltinAdd(0),
  kTfLiteBuiltinAveragePool2d(1),
  kTfLiteBuiltinConcatenation(2),
  kTfLiteBuiltinConv2d(3),
  kTfLiteBuiltinDepthwiseConv2d(4),
  kTfLiteBuiltinDepthToSpace(5),
  kTfLiteBuiltinDequantize(6),
  kTfLiteBuiltinEmbeddingLookup(7),
  kTfLiteBuiltinFloor(8),
  kTfLiteBuiltinFullyConnected(9),
  kTfLiteBuiltinHashtableLookup(10),
  kTfLiteBuiltinL2Normalization(11),
  kTfLiteBuiltinL2Pool2d(12),
  kTfLiteBuiltinLocalResponseNormalization(13),
  kTfLiteBuiltinLogistic(14),
  kTfLiteBuiltinLshProjection(15),
  kTfLiteBuiltinLstm(16),
  kTfLiteBuiltinMaxPool2d(17),
  kTfLiteBuiltinMul(18),
  kTfLiteBuiltinRelu(19),
  kTfLiteBuiltinReluN1To1(20),
  kTfLiteBuiltinRelu6(21),
  kTfLiteBuiltinReshape(22),
  kTfLiteBuiltinResizeBilinear(23),
  kTfLiteBuiltinRnn(24),
  kTfLiteBuiltinSoftmax(25),
  kTfLiteBuiltinSpaceToDepth(26),
  kTfLiteBuiltinSvdf(27),
  kTfLiteBuiltinTanh(28),
  kTfLiteBuiltinConcatEmbeddings(29),
  kTfLiteBuiltinSkipGram(30),
  kTfLiteBuiltinCall(31),
  kTfLiteBuiltinCustom(32),
  kTfLiteBuiltinEmbeddingLookupSparse(33),
  kTfLiteBuiltinPad(34),
  kTfLiteBuiltinUnidirectionalSequenceRnn(35),
  kTfLiteBuiltinGather(36),
  kTfLiteBuiltinBatchToSpaceNd(37),
  kTfLiteBuiltinSpaceToBatchNd(38),
  kTfLiteBuiltinTranspose(39),
  kTfLiteBuiltinMean(40),
  kTfLiteBuiltinSub(41),
  kTfLiteBuiltinDiv(42),
  kTfLiteBuiltinSqueeze(43),
  kTfLiteBuiltinUnidirectionalSequenceLstm(44),
  kTfLiteBuiltinStridedSlice(45),
  kTfLiteBuiltinBidirectionalSequenceRnn(46),
  kTfLiteBuiltinExp(47),
  kTfLiteBuiltinTopkV2(48),
  kTfLiteBuiltinSplit(49),
  kTfLiteBuiltinLogSoftmax(50),
  kTfLiteBuiltinDelegate(51),
  kTfLiteBuiltinBidirectionalSequenceLstm(52),
  kTfLiteBuiltinCast(53),
  kTfLiteBuiltinPrelu(54),
  kTfLiteBuiltinMaximum(55),
  kTfLiteBuiltinArgMax(56),
  kTfLiteBuiltinMinimum(57),
  kTfLiteBuiltinLess(58),
  kTfLiteBuiltinNeg(59),
  kTfLiteBuiltinPadv2(60),
  kTfLiteBuiltinGreater(61),
  kTfLiteBuiltinGreaterEqual(62),
  kTfLiteBuiltinLessEqual(63),
  kTfLiteBuiltinSelect(64),
  kTfLiteBuiltinSlice(65),
  kTfLiteBuiltinSin(66),
  kTfLiteBuiltinTransposeConv(67),
  kTfLiteBuiltinSparseToDense(68),
  kTfLiteBuiltinTile(69),
  kTfLiteBuiltinExpandDims(70),
  kTfLiteBuiltinEqual(71),
  kTfLiteBuiltinNotEqual(72),
  kTfLiteBuiltinLog(73),
  kTfLiteBuiltinSum(74),
  kTfLiteBuiltinSqrt(75),
  kTfLiteBuiltinRsqrt(76),
  kTfLiteBuiltinShape(77),
  kTfLiteBuiltinPow(78),
  kTfLiteBuiltinArgMin(79),
  kTfLiteBuiltinFakeQuant(80),
  kTfLiteBuiltinReduceProd(81),
  kTfLiteBuiltinReduceMax(82),
  kTfLiteBuiltinPack(83),
  kTfLiteBuiltinLogicalOr(84),
  kTfLiteBuiltinOneHot(85),
  kTfLiteBuiltinLogicalAnd(86),
  kTfLiteBuiltinLogicalNot(87),
  kTfLiteBuiltinUnpack(88),
  kTfLiteBuiltinReduceMin(89),
  kTfLiteBuiltinFloorDiv(90),
  kTfLiteBuiltinReduceAny(91),
  kTfLiteBuiltinSquare(92),
  kTfLiteBuiltinZerosLike(93),
  kTfLiteBuiltinFill(94),
  kTfLiteBuiltinFloorMod(95),
  kTfLiteBuiltinRange(96),
  kTfLiteBuiltinResizeNearestNeighbor(97),
  kTfLiteBuiltinLeakyRelu(98),
  kTfLiteBuiltinSquaredDifference(99),
  kTfLiteBuiltinMirrorPad(100),
  kTfLiteBuiltinAbs(101),
  kTfLiteBuiltinSplitV(102),
  kTfLiteBuiltinUnique(103),
  kTfLiteBuiltinCeil(104),
  kTfLiteBuiltinReverseV2(105),
  kTfLiteBuiltinAddN(106),
  kTfLiteBuiltinGatherNd(107),
  kTfLiteBuiltinCos(108),
  kTfLiteBuiltinWhere(109),
  kTfLiteBuiltinRank(110),
  kTfLiteBuiltinElu(111),
  kTfLiteBuiltinReverseSequence(112),
  kTfLiteBuiltinMatrixDiag(113),
  kTfLiteBuiltinQuantize(114),
  kTfLiteBuiltinMatrixSetDiag(115),
  kTfLiteBuiltinRound(116),
  kTfLiteBuiltinHardSwish(117),
  kTfLiteBuiltinIf(118),
  kTfLiteBuiltinWhile(119),
  kTfLiteBuiltinNonMaxSuppressionV4(120),
  kTfLiteBuiltinNonMaxSuppressionV5(121),
  kTfLiteBuiltinScatterNd(122),
  kTfLiteBuiltinSelectV2(123),
  kTfLiteBuiltinDensify(124),
  kTfLiteBuiltinSegmentSum(125),
  kTfLiteBuiltinBatchMatmul(126),
  kTfLiteBuiltinPlaceholderForGreaterOpCodes(127),
  kTfLiteBuiltinCumsum(128),
  kTfLiteBuiltinCallOnce(129),
  kTfLiteBuiltinBroadcastTo(130),
  kTfLiteBuiltinRfft2d(131),
  kTfLiteBuiltinConv3d(132),
  kTfLiteBuiltinImag(133),
  kTfLiteBuiltinReal(134),
  kTfLiteBuiltinComplexAbs(135),
  kTfLiteBuiltinHashtable(136),
  kTfLiteBuiltinHashtableFind(137),
  kTfLiteBuiltinHashtableImport(138),
  kTfLiteBuiltinHashtableSize(139),
  kTfLiteBuiltinReduceAll(140),
  kTfLiteBuiltinConv3dTranspose(141),
  kTfLiteBuiltinVarHandle(142),
  kTfLiteBuiltinReadVariable(143),
  kTfLiteBuiltinAssignVariable(144),
  kTfLiteBuiltinBroadcastArgs(145),
  kTfLiteBuiltinRandomStandardNormal(146),
  kTfLiteBuiltinBucketize(147),
  kTfLiteBuiltinRandomUniform(148),
  kTfLiteBuiltinMultinomial(149),
  kTfLiteBuiltinGelu(150),
  kTfLiteBuiltinDynamicUpdateSlice(151),
  kTfLiteBuiltinRelu0To1(152),
  kTfLiteBuiltinUnsortedSegmentProd(153),
  kTfLiteBuiltinUnsortedSegmentMax(154),
  kTfLiteBuiltinUnsortedSegmentSum(155),
  kTfLiteBuiltinAtan2(156),
  kTfLiteBuiltinUnsortedSegmentMin(157),
  kTfLiteBuiltinSign(158),
  kTfLiteBuiltinBitcast(159),
  kTfLiteBuiltinBitwiseXor(160),
  kTfLiteBuiltinRightShift(161),
  kTfLiteBuiltinStablehloLogistic(162),
  kTfLiteBuiltinStablehloAdd(163),
  kTfLiteBuiltinStablehloDivide(164),
  kTfLiteBuiltinStablehloMultiply(165),
  kTfLiteBuiltinStablehloMaximum(166),
  kTfLiteBuiltinStablehloReshape(167),
  kTfLiteBuiltinStablehloClamp(168),
  kTfLiteBuiltinStablehloConcatenate(169),
  kTfLiteBuiltinStablehloBroadcastInDim(170),
  kTfLiteBuiltinStablehloConvolution(171),
  kTfLiteBuiltinStablehloSlice(172),
  kTfLiteBuiltinStablehloCustomCall(173),
  kTfLiteBuiltinStablehloReduce(174),
  kTfLiteBuiltinStablehloAbs(175),
  kTfLiteBuiltinStablehloAnd(176),
  kTfLiteBuiltinStablehloCosine(177),
  kTfLiteBuiltinStablehloExponential(178),
  kTfLiteBuiltinStablehloFloor(179),
  kTfLiteBuiltinStablehloLog(180),
  kTfLiteBuiltinStablehloMinimum(181),
  kTfLiteBuiltinStablehloNegate(182),
  kTfLiteBuiltinStablehloOr(183),
  kTfLiteBuiltinStablehloPower(184),
  kTfLiteBuiltinStablehloRemainder(185),
  kTfLiteBuiltinStablehloRsqrt(186),
  kTfLiteBuiltinStablehloSelect(187),
  kTfLiteBuiltinStablehloSubtract(188),
  kTfLiteBuiltinStablehloTanh(189),
  kTfLiteBuiltinStablehloScatter(190),
  kTfLiteBuiltinStablehloCompare(191),
  kTfLiteBuiltinStablehloConvert(192),
  kTfLiteBuiltinStablehloDynamicSlice(193),
  kTfLiteBuiltinStablehloDynamicUpdateSlice(194),
  kTfLiteBuiltinStablehloPad(195),
  kTfLiteBuiltinStablehloIota(196),
  kTfLiteBuiltinStablehloDotGeneral(197),
  kTfLiteBuiltinStablehloReduceWindow(198),
  kTfLiteBuiltinStablehloSort(199),
  kTfLiteBuiltinStablehloWhile(200),
  kTfLiteBuiltinStablehloGather(201),
  kTfLiteBuiltinStablehloTranspose(202),
  kTfLiteBuiltinDilate(203),
  kTfLiteBuiltinStablehloRngBitGenerator(204),
  kTfLiteBuiltinReduceWindow(205),
  kTfLiteBuiltinStablehloComposite(206),
  kTfLiteBuiltinStablehloShiftLeft(207),
  kTfLiteBuiltinStablehloCbrt(208),
  kTfLiteBuiltinStablehloCase(209);

  final int value;
  const TfLiteBuiltinOperator(this.value);

  static TfLiteBuiltinOperator fromValue(int value) => switch (value) {
        0 => kTfLiteBuiltinAdd,
        1 => kTfLiteBuiltinAveragePool2d,
        2 => kTfLiteBuiltinConcatenation,
        3 => kTfLiteBuiltinConv2d,
        4 => kTfLiteBuiltinDepthwiseConv2d,
        5 => kTfLiteBuiltinDepthToSpace,
        6 => kTfLiteBuiltinDequantize,
        7 => kTfLiteBuiltinEmbeddingLookup,
        8 => kTfLiteBuiltinFloor,
        9 => kTfLiteBuiltinFullyConnected,
        10 => kTfLiteBuiltinHashtableLookup,
        11 => kTfLiteBuiltinL2Normalization,
        12 => kTfLiteBuiltinL2Pool2d,
        13 => kTfLiteBuiltinLocalResponseNormalization,
        14 => kTfLiteBuiltinLogistic,
        15 => kTfLiteBuiltinLshProjection,
        16 => kTfLiteBuiltinLstm,
        17 => kTfLiteBuiltinMaxPool2d,
        18 => kTfLiteBuiltinMul,
        19 => kTfLiteBuiltinRelu,
        20 => kTfLiteBuiltinReluN1To1,
        21 => kTfLiteBuiltinRelu6,
        22 => kTfLiteBuiltinReshape,
        23 => kTfLiteBuiltinResizeBilinear,
        24 => kTfLiteBuiltinRnn,
        25 => kTfLiteBuiltinSoftmax,
        26 => kTfLiteBuiltinSpaceToDepth,
        27 => kTfLiteBuiltinSvdf,
        28 => kTfLiteBuiltinTanh,
        29 => kTfLiteBuiltinConcatEmbeddings,
        30 => kTfLiteBuiltinSkipGram,
        31 => kTfLiteBuiltinCall,
        32 => kTfLiteBuiltinCustom,
        33 => kTfLiteBuiltinEmbeddingLookupSparse,
        34 => kTfLiteBuiltinPad,
        35 => kTfLiteBuiltinUnidirectionalSequenceRnn,
        36 => kTfLiteBuiltinGather,
        37 => kTfLiteBuiltinBatchToSpaceNd,
        38 => kTfLiteBuiltinSpaceToBatchNd,
        39 => kTfLiteBuiltinTranspose,
        40 => kTfLiteBuiltinMean,
        41 => kTfLiteBuiltinSub,
        42 => kTfLiteBuiltinDiv,
        43 => kTfLiteBuiltinSqueeze,
        44 => kTfLiteBuiltinUnidirectionalSequenceLstm,
        45 => kTfLiteBuiltinStridedSlice,
        46 => kTfLiteBuiltinBidirectionalSequenceRnn,
        47 => kTfLiteBuiltinExp,
        48 => kTfLiteBuiltinTopkV2,
        49 => kTfLiteBuiltinSplit,
        50 => kTfLiteBuiltinLogSoftmax,
        51 => kTfLiteBuiltinDelegate,
        52 => kTfLiteBuiltinBidirectionalSequenceLstm,
        53 => kTfLiteBuiltinCast,
        54 => kTfLiteBuiltinPrelu,
        55 => kTfLiteBuiltinMaximum,
        56 => kTfLiteBuiltinArgMax,
        57 => kTfLiteBuiltinMinimum,
        58 => kTfLiteBuiltinLess,
        59 => kTfLiteBuiltinNeg,
        60 => kTfLiteBuiltinPadv2,
        61 => kTfLiteBuiltinGreater,
        62 => kTfLiteBuiltinGreaterEqual,
        63 => kTfLiteBuiltinLessEqual,
        64 => kTfLiteBuiltinSelect,
        65 => kTfLiteBuiltinSlice,
        66 => kTfLiteBuiltinSin,
        67 => kTfLiteBuiltinTransposeConv,
        68 => kTfLiteBuiltinSparseToDense,
        69 => kTfLiteBuiltinTile,
        70 => kTfLiteBuiltinExpandDims,
        71 => kTfLiteBuiltinEqual,
        72 => kTfLiteBuiltinNotEqual,
        73 => kTfLiteBuiltinLog,
        74 => kTfLiteBuiltinSum,
        75 => kTfLiteBuiltinSqrt,
        76 => kTfLiteBuiltinRsqrt,
        77 => kTfLiteBuiltinShape,
        78 => kTfLiteBuiltinPow,
        79 => kTfLiteBuiltinArgMin,
        80 => kTfLiteBuiltinFakeQuant,
        81 => kTfLiteBuiltinReduceProd,
        82 => kTfLiteBuiltinReduceMax,
        83 => kTfLiteBuiltinPack,
        84 => kTfLiteBuiltinLogicalOr,
        85 => kTfLiteBuiltinOneHot,
        86 => kTfLiteBuiltinLogicalAnd,
        87 => kTfLiteBuiltinLogicalNot,
        88 => kTfLiteBuiltinUnpack,
        89 => kTfLiteBuiltinReduceMin,
        90 => kTfLiteBuiltinFloorDiv,
        91 => kTfLiteBuiltinReduceAny,
        92 => kTfLiteBuiltinSquare,
        93 => kTfLiteBuiltinZerosLike,
        94 => kTfLiteBuiltinFill,
        95 => kTfLiteBuiltinFloorMod,
        96 => kTfLiteBuiltinRange,
        97 => kTfLiteBuiltinResizeNearestNeighbor,
        98 => kTfLiteBuiltinLeakyRelu,
        99 => kTfLiteBuiltinSquaredDifference,
        100 => kTfLiteBuiltinMirrorPad,
        101 => kTfLiteBuiltinAbs,
        102 => kTfLiteBuiltinSplitV,
        103 => kTfLiteBuiltinUnique,
        104 => kTfLiteBuiltinCeil,
        105 => kTfLiteBuiltinReverseV2,
        106 => kTfLiteBuiltinAddN,
        107 => kTfLiteBuiltinGatherNd,
        108 => kTfLiteBuiltinCos,
        109 => kTfLiteBuiltinWhere,
        110 => kTfLiteBuiltinRank,
        111 => kTfLiteBuiltinElu,
        112 => kTfLiteBuiltinReverseSequence,
        113 => kTfLiteBuiltinMatrixDiag,
        114 => kTfLiteBuiltinQuantize,
        115 => kTfLiteBuiltinMatrixSetDiag,
        116 => kTfLiteBuiltinRound,
        117 => kTfLiteBuiltinHardSwish,
        118 => kTfLiteBuiltinIf,
        119 => kTfLiteBuiltinWhile,
        120 => kTfLiteBuiltinNonMaxSuppressionV4,
        121 => kTfLiteBuiltinNonMaxSuppressionV5,
        122 => kTfLiteBuiltinScatterNd,
        123 => kTfLiteBuiltinSelectV2,
        124 => kTfLiteBuiltinDensify,
        125 => kTfLiteBuiltinSegmentSum,
        126 => kTfLiteBuiltinBatchMatmul,
        127 => kTfLiteBuiltinPlaceholderForGreaterOpCodes,
        128 => kTfLiteBuiltinCumsum,
        129 => kTfLiteBuiltinCallOnce,
        130 => kTfLiteBuiltinBroadcastTo,
        131 => kTfLiteBuiltinRfft2d,
        132 => kTfLiteBuiltinConv3d,
        133 => kTfLiteBuiltinImag,
        134 => kTfLiteBuiltinReal,
        135 => kTfLiteBuiltinComplexAbs,
        136 => kTfLiteBuiltinHashtable,
        137 => kTfLiteBuiltinHashtableFind,
        138 => kTfLiteBuiltinHashtableImport,
        139 => kTfLiteBuiltinHashtableSize,
        140 => kTfLiteBuiltinReduceAll,
        141 => kTfLiteBuiltinConv3dTranspose,
        142 => kTfLiteBuiltinVarHandle,
        143 => kTfLiteBuiltinReadVariable,
        144 => kTfLiteBuiltinAssignVariable,
        145 => kTfLiteBuiltinBroadcastArgs,
        146 => kTfLiteBuiltinRandomStandardNormal,
        147 => kTfLiteBuiltinBucketize,
        148 => kTfLiteBuiltinRandomUniform,
        149 => kTfLiteBuiltinMultinomial,
        150 => kTfLiteBuiltinGelu,
        151 => kTfLiteBuiltinDynamicUpdateSlice,
        152 => kTfLiteBuiltinRelu0To1,
        153 => kTfLiteBuiltinUnsortedSegmentProd,
        154 => kTfLiteBuiltinUnsortedSegmentMax,
        155 => kTfLiteBuiltinUnsortedSegmentSum,
        156 => kTfLiteBuiltinAtan2,
        157 => kTfLiteBuiltinUnsortedSegmentMin,
        158 => kTfLiteBuiltinSign,
        159 => kTfLiteBuiltinBitcast,
        160 => kTfLiteBuiltinBitwiseXor,
        161 => kTfLiteBuiltinRightShift,
        162 => kTfLiteBuiltinStablehloLogistic,
        163 => kTfLiteBuiltinStablehloAdd,
        164 => kTfLiteBuiltinStablehloDivide,
        165 => kTfLiteBuiltinStablehloMultiply,
        166 => kTfLiteBuiltinStablehloMaximum,
        167 => kTfLiteBuiltinStablehloReshape,
        168 => kTfLiteBuiltinStablehloClamp,
        169 => kTfLiteBuiltinStablehloConcatenate,
        170 => kTfLiteBuiltinStablehloBroadcastInDim,
        171 => kTfLiteBuiltinStablehloConvolution,
        172 => kTfLiteBuiltinStablehloSlice,
        173 => kTfLiteBuiltinStablehloCustomCall,
        174 => kTfLiteBuiltinStablehloReduce,
        175 => kTfLiteBuiltinStablehloAbs,
        176 => kTfLiteBuiltinStablehloAnd,
        177 => kTfLiteBuiltinStablehloCosine,
        178 => kTfLiteBuiltinStablehloExponential,
        179 => kTfLiteBuiltinStablehloFloor,
        180 => kTfLiteBuiltinStablehloLog,
        181 => kTfLiteBuiltinStablehloMinimum,
        182 => kTfLiteBuiltinStablehloNegate,
        183 => kTfLiteBuiltinStablehloOr,
        184 => kTfLiteBuiltinStablehloPower,
        185 => kTfLiteBuiltinStablehloRemainder,
        186 => kTfLiteBuiltinStablehloRsqrt,
        187 => kTfLiteBuiltinStablehloSelect,
        188 => kTfLiteBuiltinStablehloSubtract,
        189 => kTfLiteBuiltinStablehloTanh,
        190 => kTfLiteBuiltinStablehloScatter,
        191 => kTfLiteBuiltinStablehloCompare,
        192 => kTfLiteBuiltinStablehloConvert,
        193 => kTfLiteBuiltinStablehloDynamicSlice,
        194 => kTfLiteBuiltinStablehloDynamicUpdateSlice,
        195 => kTfLiteBuiltinStablehloPad,
        196 => kTfLiteBuiltinStablehloIota,
        197 => kTfLiteBuiltinStablehloDotGeneral,
        198 => kTfLiteBuiltinStablehloReduceWindow,
        199 => kTfLiteBuiltinStablehloSort,
        200 => kTfLiteBuiltinStablehloWhile,
        201 => kTfLiteBuiltinStablehloGather,
        202 => kTfLiteBuiltinStablehloTranspose,
        203 => kTfLiteBuiltinDilate,
        204 => kTfLiteBuiltinStablehloRngBitGenerator,
        205 => kTfLiteBuiltinReduceWindow,
        206 => kTfLiteBuiltinStablehloComposite,
        207 => kTfLiteBuiltinStablehloShiftLeft,
        208 => kTfLiteBuiltinStablehloCbrt,
        209 => kTfLiteBuiltinStablehloCase,
        _ => throw ArgumentError(
            "Unknown value for TfLiteBuiltinOperator: $value"),
      };
}

final class TfLiteAsyncKernel extends ffi.Opaque {}

final class TfLiteExecutionTask extends ffi.Opaque {}

/// Enum tag for specifying whether a tensor is the input or output to the
/// model.
enum TfLiteIoType {
  kTfLiteIoTypeUnknown(0),
  kTfLiteIoTypeInput(1),
  kTfLiteIoTypeOutput(2);

  final int value;
  const TfLiteIoType(this.value);

  static TfLiteIoType fromValue(int value) => switch (value) {
        0 => kTfLiteIoTypeUnknown,
        1 => kTfLiteIoTypeInput,
        2 => kTfLiteIoTypeOutput,
        _ => throw ArgumentError("Unknown value for TfLiteIoType: $value"),
      };
}

/// Types supported by tensor
/// LINT.IfChange
enum TfLiteType {
  kTfLiteNoType(0),
  kTfLiteFloat32(1),
  kTfLiteInt32(2),
  kTfLiteUInt8(3),
  kTfLiteInt64(4),
  kTfLiteString(5),
  kTfLiteBool(6),
  kTfLiteInt16(7),
  kTfLiteComplex64(8),
  kTfLiteInt8(9),
  kTfLiteFloat16(10),
  kTfLiteFloat64(11),
  kTfLiteComplex128(12),
  kTfLiteUInt64(13),
  kTfLiteResource(14),
  kTfLiteVariant(15),
  kTfLiteUInt32(16),
  kTfLiteUInt16(17),
  kTfLiteInt4(18),
  kTfLiteBFloat16(19),
  kTfLiteInt2(20);

  final int value;
  const TfLiteType(this.value);

  static TfLiteType fromValue(int value) => switch (value) {
        0 => kTfLiteNoType,
        1 => kTfLiteFloat32,
        2 => kTfLiteInt32,
        3 => kTfLiteUInt8,
        4 => kTfLiteInt64,
        5 => kTfLiteString,
        6 => kTfLiteBool,
        7 => kTfLiteInt16,
        8 => kTfLiteComplex64,
        9 => kTfLiteInt8,
        10 => kTfLiteFloat16,
        11 => kTfLiteFloat64,
        12 => kTfLiteComplex128,
        13 => kTfLiteUInt64,
        14 => kTfLiteResource,
        15 => kTfLiteVariant,
        16 => kTfLiteUInt32,
        17 => kTfLiteUInt16,
        18 => kTfLiteInt4,
        19 => kTfLiteBFloat16,
        20 => kTfLiteInt2,
        _ => throw ArgumentError("Unknown value for TfLiteType: $value"),
      };
}

/// Legacy. Will be deprecated in favor of `TfLiteAffineQuantization`.
/// If per-layer quantization is specified this field will still be populated in
/// addition to `TfLiteAffineQuantization`.
/// Parameters for asymmetric quantization. Quantized values can be converted
/// back to float using: `real_value = scale * (quantized_value - zero_point)`
final class TfLiteQuantizationParams extends ffi.Struct {
  @ffi.Float()
  external double scale;

  @ffi.Int32()
  external int zero_point;
}

/// Storage format of each dimension in a sparse tensor.
enum TfLiteDimensionType {
  kTfLiteDimDense(0),
  kTfLiteDimSparseCSR(1);

  final int value;
  const TfLiteDimensionType(this.value);

  static TfLiteDimensionType fromValue(int value) => switch (value) {
        0 => kTfLiteDimDense,
        1 => kTfLiteDimSparseCSR,
        _ =>
          throw ArgumentError("Unknown value for TfLiteDimensionType: $value"),
      };
}

/// Note that new error status values may be added in future in order to
/// indicate more fine-grained internal states, therefore, applications should
/// not rely on status values being members of the enum.
enum TfLiteStatus {
  /// Success
  kTfLiteOk(0),

  /// Generally referring to an error in the runtime (i.e. interpreter)
  kTfLiteError(1),

  /// Generally referring to an error from a TfLiteDelegate itself.
  kTfLiteDelegateError(2),

  /// Generally referring to an error in applying a delegate due to
  /// incompatibility between runtime and delegate, e.g., this error is returned
  /// when trying to apply a TF Lite delegate onto a model graph that's already
  /// immutable.
  kTfLiteApplicationError(3),

  /// Generally referring to serialized delegate data not being found.
  /// See tflite::delegates::Serialization.
  kTfLiteDelegateDataNotFound(4),

  /// Generally referring to data-writing issues in delegate serialization.
  /// See tflite::delegates::Serialization.
  kTfLiteDelegateDataWriteError(5),

  /// Generally referring to data-reading issues in delegate serialization.
  /// See tflite::delegates::Serialization.
  kTfLiteDelegateDataReadError(6),

  /// Generally referring to issues when the TF Lite model has ops that cannot
  /// be resolved at runtime. This could happen when the specific op is not
  /// registered or built with the TF Lite framework.
  kTfLiteUnresolvedOps(7),

  /// Generally referring to invocation cancelled by the user.
  /// See `interpreter::Cancel`.
  /// TODO(b/194915839): Implement `interpreter::Cancel`.
  /// TODO(b/250636993): Cancellation triggered by `SetCancellationFunction`
  /// should also return this status code.
  kTfLiteCancelled(8),

  /// This status is returned by Prepare when the output shape cannot be
  /// determined but the size of the output tensor is known. For example, the
  /// output of reshape is always the same size as the input. This means that
  /// such ops may be
  /// done in place.
  kTfLiteOutputShapeNotKnown(9);

  final int value;
  const TfLiteStatus(this.value);

  static TfLiteStatus fromValue(int value) => switch (value) {
        0 => kTfLiteOk,
        1 => kTfLiteError,
        2 => kTfLiteDelegateError,
        3 => kTfLiteApplicationError,
        4 => kTfLiteDelegateDataNotFound,
        5 => kTfLiteDelegateDataWriteError,
        6 => kTfLiteDelegateDataReadError,
        7 => kTfLiteUnresolvedOps,
        8 => kTfLiteCancelled,
        9 => kTfLiteOutputShapeNotKnown,
        _ => throw ArgumentError("Unknown value for TfLiteStatus: $value"),
      };
}

final class TfLiteOpaqueContext extends ffi.Opaque {}

final class TfLiteOpaqueNode extends ffi.Opaque {}

final class TfLiteOpaqueTensor extends ffi.Opaque {}

/// WARNING: This is an experimental interface that is subject to change.
final class TfLiteDelegate extends ffi.Struct {
  /// Data that delegate needs to identify itself. This data is owned by the
  /// delegate. The delegate is owned in the user code, so the delegate is
  /// responsible for deallocating this when it is destroyed.
  external ffi.Pointer<ffi.Void> data_;

  /// Invoked by `ModifyGraphWithDelegate`. This prepare is called, giving the
  /// delegate a view of the current graph through `TfLiteContext*`. It
  /// typically will look at the nodes and call
  /// `ReplaceNodeSubsetsWithDelegateKernels()` to ask the TensorFlow lite
  /// runtime to create macro-nodes to represent delegated subgraphs of the
  /// original graph.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate)>> Prepare;

  /// Copy the data from delegate buffer handle into raw memory of the given
  /// `tensor`. Note that the delegate is allowed to allocate the raw bytes as
  /// long as it follows the rules for `kTfLiteDynamic` tensors, in which case
  /// this cannot be null.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteTensor> tensor)>> CopyFromBufferHandle;

  /// Copy the data from raw memory of the given `tensor` to delegate buffer
  /// handle. This can be null if the delegate doesn't use its own buffer.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteTensor> tensor)>> CopyToBufferHandle;

  /// Free the Delegate Buffer Handle. Note: This only frees the handle, but
  /// this doesn't release the underlying resource (e.g. textures). The
  /// resources are either owned by application layer or the delegate.
  /// This can be null if the delegate doesn't use its own buffer.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteDelegate> delegate,
              ffi.Pointer<TfLiteBufferHandle> handle)>> FreeBufferHandle;

  /// Bitmask flags. See the comments in `TfLiteDelegateFlags`.
  @ffi.Int64()
  external int flags;

  /// The opaque delegate builder associated with this object.  If set then the
  /// TF Lite runtime will give precedence to this field.  E.g. instead of
  /// invoking `Prepare` via the function pointer inside the `TfLiteDelegate`
  /// object, the runtime will first check if the corresponding function
  /// pointer inside `opaque_delegate_builder` is set and if so invoke that.
  ///
  /// If this field is non-null, then the `Prepare` field (of the
  /// `TfLiteDelegate`) should be null.
  external ffi.Pointer<TfLiteOpaqueDelegateBuilder> opaque_delegate_builder;
}

/// `TfLiteContext` allows an op to access the tensors.
///
/// `TfLiteContext` is a struct that is created by the TF Lite runtime
/// and passed to the "methods" (C function pointers) in the
/// `TfLiteRegistration` struct that are used to define custom ops and custom
/// delegate kernels. It contains information and methods (C function pointers)
/// that can be called by the code implementing a custom op or a custom delegate
/// kernel. These methods provide access to the context in which that custom op
/// or custom delegate kernel occurs, such as access to the input and output
/// tensors for that op, as well as methods for allocating memory buffers
/// and intermediate tensors, etc.
///
/// See also `TfLiteOpaqueContext`, which is an more ABI-stable equivalent.
final class TfLiteContext extends ffi.Struct {
  /// Number of tensors in the context.
  @ffi.Size()
  external int tensors_size;

  /// The execution plan contains a list of the node indices in execution
  /// order. execution_plan->size is the current number of nodes. And,
  /// execution_plan->data[0] is the first node that needs to be run.
  /// TfLiteDelegates can traverse the current execution plan by iterating
  /// through each member of this array and using GetNodeAndRegistration() to
  /// access details about a node. i.e.
  ///
  ///
  /// TfLiteIntArray* execution_plan;
  /// TF_LITE_ENSURE_STATUS(context->GetExecutionPlan(context,
  /// &execution_plan));
  /// for (int exec_index = 0; exec_index < execution_plan->size;
  /// exec_index++) {
  /// int node_index = execution_plan->data[exec_index];
  /// TfLiteNode* node;
  /// TfLiteRegistration* reg;
  /// context->GetNodeAndRegistration(context, node_index, &node, &reg);
  /// }
  ///
  /// Note: the memory pointed by '`*execution_plan` is OWNED by TfLite runtime.
  /// Future calls to GetExecutionPlan invalidates earlier outputs. The
  /// following code snippet shows the issue of such an invocation pattern.
  /// After calling CheckNode, subsequent access to `plan_1st` is undefined.
  ///
  /// void CheckNode(const TfLiteNode* node) {
  /// ...
  /// TfLiteIntArray* plan_2nd;
  /// TF_LITE_ENSURE_STATUS(
  /// context->GetExecutionPlan(context, &plan_2nd)
  /// );
  /// ...
  /// }
  ///
  /// TfLiteIntArray* plan_1st;
  /// TF_LITE_ENSURE_STATUS(context->GetExecutionPlan(context, &plan_1st));
  /// for (int exec_index = 0; exec_index < plan_1st->size; exec_index++) {
  /// int node_index = plan_1st->data[exec_index];
  /// TfLiteNode* node;
  /// TfLiteRegistration* reg;
  /// context->GetNodeAndRegistration(context, node_index, &node, &reg);
  /// CheckNode(node);
  /// }
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
                  ffi.Pointer<ffi.Pointer<TfLiteIntArray>> execution_plan)>>
      GetExecutionPlan;

  /// An array of tensors in the interpreter context (of length `tensors_size`)
  external ffi.Pointer<TfLiteTensor> tensors;

  /// opaque full context ptr (an opaque c++ data structure)
  external ffi.Pointer<ffi.Void> impl_;

  /// Request memory pointer be resized. Updates dimensions on the tensor.
  /// NOTE: ResizeTensor takes ownership of newSize.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext>,
              ffi.Pointer<TfLiteTensor>,
              ffi.Pointer<TfLiteIntArray>)>> ResizeTensor;

  /// Request that an error be reported with format string msg.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteContext>, ffi.Pointer<ffi.Char>)>> ReportError;

  /// Add `tensors_to_add` tensors, preserving pre-existing Tensor entries.  If
  /// non-null, the value pointed to by `first_new_tensor_index` will be set to
  /// the index of the first new tensor.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext>, ffi.Int, ffi.Pointer<ffi.Int>)>>
      AddTensors;

  /// Get a Tensor node by node_index.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext>,
                  ffi.Int,
                  ffi.Pointer<ffi.Pointer<TfLiteNode>>,
                  ffi.Pointer<ffi.Pointer<TfLiteRegistration>>)>>
      GetNodeAndRegistration;

  /// Replace ops with one or more stub delegate operations. This function
  /// does not take ownership of `nodes_to_replace`.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext>,
                  TfLiteRegistration,
                  ffi.Pointer<TfLiteIntArray>,
                  ffi.Pointer<TfLiteDelegate>)>>
      ReplaceNodeSubsetsWithDelegateKernels;

  /// Number of threads that are recommended to subsystems like gemmlowp and
  /// eigen.
  @ffi.Int()
  external int recommended_num_threads;

  /// Access external contexts by type.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteExternalContext> Function(
              ffi.Pointer<TfLiteContext>, ffi.UnsignedInt)>> GetExternalContext;

  /// Set the value of a external context. Does not take ownership of the
  /// pointer.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext>, ffi.UnsignedInt,
              ffi.Pointer<TfLiteExternalContext>)>> SetExternalContext;

  /// Flag for allowing float16 precision for FP32 calculation.
  /// default: false.
  ///
  /// WARNING: This is an experimental API and subject to change.
  @ffi.Bool()
  external bool allow_fp32_relax_to_fp16;

  /// Pointer to the op-level profiler, if set; nullptr otherwise.
  external ffi.Pointer<ffi.Void> profiler;

  /// Allocate persistent buffer which has the same life time as the
  /// interpreter. Returns `nullptr` on failure. The memory is allocated from
  /// heap for TFL, and from tail in TFLM. This method is only available in
  /// `Init` or `Prepare` stage.
  ///
  /// WARNING: This is an experimental interface that is subject
  /// to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(
                  ffi.Pointer<TfLiteContext> ctx, ffi.Size bytes)>>
      AllocatePersistentBuffer;

  /// Allocate a buffer which will be deallocated right after invoke phase.
  /// The memory is allocated from heap in TFL, and from volatile arena in TFLM.
  /// This method is only available in invoke stage.
  ///
  /// NOTE: If possible use `RequestScratchBufferInArena` method to avoid memory
  /// allocation during inference time.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> ctx,
              ffi.Size bytes,
              ffi.Pointer<ffi.Pointer<ffi.Void>> ptr)>> AllocateBufferForEval;

  /// Request a scratch buffer in the arena through static memory planning.
  /// This method is only available in `Prepare` stage and the buffer is
  /// allocated by the interpreter between Prepare and Eval stage. In `Eval`
  /// stage, `GetScratchBuffer` API can be used to fetch the address.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> ctx,
              ffi.Size bytes,
              ffi.Pointer<ffi.Int> buffer_idx)>> RequestScratchBufferInArena;

  /// Get the scratch buffer pointer.
  /// This method is only available in Eval stage.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Void> Function(
                  ffi.Pointer<TfLiteContext> ctx, ffi.Int buffer_idx)>>
      GetScratchBuffer;

  /// Resize the memory pointer of the `tensor`. This method behaves the same as
  /// `ResizeTensor`, except that it makes a copy of the shape array internally
  /// so the shape array could be deallocated right afterwards.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> ctx,
              ffi.Pointer<TfLiteTensor> tensor,
              ffi.Int dims,
              ffi.Pointer<ffi.Int> shape)>> ResizeTensorExplicit;

  /// This method provides a preview of post-delegation partitioning. Each
  /// TfLiteDelegateParams in the referenced array corresponds to one instance
  /// of the delegate kernel. Example usage:
  ///
  /// TfLiteIntArray* nodes_to_replace = ...;
  /// TfLiteDelegateParams* params_array;
  /// int num_partitions = 0;
  /// TF_LITE_ENSURE_STATUS(context->PreviewDelegatePartitioning(
  /// context, delegate, nodes_to_replace, &params_array,
  /// &num_partitions));
  /// for (int idx = 0; idx < num_partitions; idx++) {
  /// const auto& partition_params = params_array[idx];
  /// ...
  /// }
  ///
  /// NOTE: The context owns the memory referenced by partition_params_array. It
  /// will be cleared with another call to PreviewDelegatePartitioning, or after
  /// TfLiteDelegateParams::Prepare returns.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext> context,
                  ffi.Pointer<TfLiteIntArray> nodes_to_replace,
                  ffi.Pointer<ffi.Pointer<TfLiteDelegateParams>>
                      partition_params_array,
                  ffi.Pointer<ffi.Int> num_partitions)>>
      PreviewDelegatePartitioning;

  /// Returns a TfLiteTensor struct for a given index.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  ///
  /// WARNING: This method may not be available on all platforms.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteTensor> Function(
                  ffi.Pointer<TfLiteContext> context, ffi.Int tensor_idx)>>
      GetTensor;

  /// Returns a TfLiteEvalTensor struct for a given index.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  ///
  /// WARNING: This method may not be available on all platforms.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<TfLiteEvalTensor> Function(
                  ffi.Pointer<TfLiteContext> context, ffi.Int tensor_idx)>>
      GetEvalTensor;

  /// Retrieves named metadata buffer from the TFLite model.
  /// Returns kTfLiteOk if metadata is successfully obtained from the flatbuffer
  /// Model: that is, there exists a `metadata` entry with given `name` string.
  /// (see TFLite's schema.fbs).
  /// The corresponding `buffer` information is populated in `ptr` & `bytes`.
  /// The data from `ptr` is valid for the lifetime of the Interpreter.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> name,
              ffi.Pointer<ffi.Pointer<ffi.Char>> ptr,
              ffi.Pointer<ffi.Size> bytes)>> GetModelMetadata;

  /// Retrieves the corresponding TfLiteContext of a subgraph that the given
  /// subgraph_index points to and switches to the delegate context for that
  /// subgraph. If an invalid subgraph index is given, returns kTfLiteError.
  ///
  /// NOTE: This function is expected to be paired with ReleaseSubgraphContext()
  /// once the delegate preparation is done and/or the delegate context
  /// functions are no longer needed.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext> context,
                  ffi.Int subgraph_index,
                  ffi.Pointer<ffi.Pointer<TfLiteContext>> acquired_context)>>
      AcquireSubgraphContext;

  /// Releases the subgraph context by switching back to the TFLite kernel
  /// context for the subgraph that the given subgraph_index points to.
  ///
  /// NOTE: This function is expected to be used after AcquireSubgraphContext()
  /// once the delegate preparation is done and/or the delegate context
  /// functions are no longer needed.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(
                  ffi.Pointer<TfLiteContext> context, ffi.Int subgraph_index)>>
      ReleaseSubgraphContext;
}

/// Fixed size list of integers. Used for dimensions and inputs/outputs tensor
/// indices
final class TfLiteIntArray extends ffi.Opaque {}

final class TfLiteTensor extends ffi.Struct {
  /// The data type specification for data stored in `data`. This affects
  /// what member of `data` union should be used.
  @ffi.UnsignedInt()
  external int type;

  /// A union of data pointers. The appropriate type should be used for a typed
  /// tensor based on `type`.
  external TfLitePtrUnion data;

  /// A pointer to a structure representing the dimensionality interpretation
  /// that the buffer should have. NOTE: the product of elements of `dims`
  /// and the element datatype size should be equal to `bytes` below.
  external ffi.Pointer<TfLiteIntArray> dims;

  /// Quantization information.
  external TfLiteQuantizationParams params;

  /// How memory is mapped
  /// kTfLiteMmapRo: Memory mapped read only.
  /// i.e. weights
  /// kTfLiteArenaRw: Arena allocated read write memory
  /// (i.e. temporaries, outputs).
  @ffi.UnsignedInt()
  external int allocation_type;

  /// The number of bytes required to store the data of this Tensor. I.e.
  /// (bytes of each element) * dims[0] * ... * dims[n-1].  For example, if
  /// type is kTfLiteFloat32 and dims = {3, 2} then
  /// bytes = sizeof(float) * 3 * 2 = 4 * 3 * 2 = 24.
  @ffi.Size()
  external int bytes;

  /// An opaque pointer to a tflite::MMapAllocation
  external ffi.Pointer<ffi.Void> allocation;

  /// Null-terminated name of this tensor.
  external ffi.Pointer<ffi.Char> name;

  /// The delegate which knows how to handle `buffer_handle`.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteDelegate> delegate;

  /// An integer buffer handle that can be handled by `delegate`.
  /// The value is valid only when delegate is not null.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  @TfLiteBufferHandle()
  external int buffer_handle;

  /// If the delegate uses its own buffer (e.g. GPU memory), the delegate is
  /// responsible to set data_is_stale to true.
  /// `delegate->CopyFromBufferHandle` can be called to copy the data from
  /// delegate buffer.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  @ffi.Bool()
  external bool data_is_stale;

  /// True if the tensor is a variable.
  @ffi.Bool()
  external bool is_variable;

  /// Quantization information. Replaces params field above.
  external TfLiteQuantization quantization;

  /// Parameters used to encode a sparse tensor.
  /// This is optional. The field is NULL if a tensor is dense.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteSparsity> sparsity;

  /// Optional. Encodes shapes with unknown dimensions with -1. This field is
  /// only populated when unknown dimensions exist in a read-write tensor (i.e.
  /// an input or output tensor). (e.g.  `dims` contains [1, 1, 1, 3] and
  /// `dims_signature` contains [1, -1, -1, 3]).  If no unknown dimensions exist
  /// then `dims_signature` is either null, or set to an empty array.  Use
  /// `TfLiteTensorGetDimsSignature` to get `dims_signature` if non-empty or
  /// otherwise fallback to `dims`.  Note that this field only exists when
  /// TF_LITE_STATIC_MEMORY is not defined.
  external ffi.Pointer<TfLiteIntArray> dims_signature;
}

/// A union of pointers that points to memory for a given tensor.
///
/// Do not access these members directly, if possible, use
/// `GetTensorData<TYPE>(tensor)` instead, otherwise only access `.data`, as
/// other members are deprecated.
final class TfLitePtrUnion extends ffi.Union {
  external ffi.Pointer<ffi.Int32> i32;

  external ffi.Pointer<ffi.Uint32> u32;

  external ffi.Pointer<ffi.Int64> i64;

  external ffi.Pointer<ffi.Uint64> u64;

  external ffi.Pointer<ffi.Float> f;

  external ffi.Pointer<TfLiteFloat16> f16;

  external ffi.Pointer<TfLiteBFloat16> bf16;

  external ffi.Pointer<ffi.Double> f64;

  external ffi.Pointer<ffi.Char> raw;

  external ffi.Pointer<ffi.Char> raw_const;

  external ffi.Pointer<ffi.Uint8> uint8;

  external ffi.Pointer<ffi.Bool> b;

  external ffi.Pointer<ffi.Int16> i16;

  external ffi.Pointer<ffi.Uint16> ui16;

  external ffi.Pointer<TfLiteComplex64> c64;

  external ffi.Pointer<TfLiteComplex128> c128;

  external ffi.Pointer<ffi.Int8> int8;

  /// Only use this member.
  external ffi.Pointer<ffi.Void> data;
}

/// Half precision data type compatible with the C99 definition.
final class TfLiteFloat16 extends ffi.Struct {
  @ffi.Uint16()
  external int data;
}

/// bfloat16 data type compatible with the Google Brain definition.
/// https://cloud.google.com/tpu/docs/bfloat16.
/// This provides 1 bit of sign, 8 bits of exponent, and 7 bits of mantissa.
final class TfLiteBFloat16 extends ffi.Struct {
  @ffi.Uint16()
  external int data;
}

/// Single-precision complex data type compatible with the C99 definition.
final class TfLiteComplex64 extends ffi.Struct {
  @ffi.Float()
  external double re;

  @ffi.Float()
  external double im;
}

/// Double-precision complex data type compatible with the C99 definition.
final class TfLiteComplex128 extends ffi.Struct {
  @ffi.Double()
  external double re;

  @ffi.Double()
  external double im;
}

/// Memory allocation strategies.
/// * `kTfLiteMmapRo`: Read-only memory-mapped data, or data externally
/// allocated.
/// * `kTfLiteArenaRw`: Arena allocated with no guarantees about persistence,
/// and available during eval.
/// * `kTfLiteArenaRwPersistent`: Arena allocated but persistent across eval,
/// and only available during eval.
/// * `kTfLiteDynamic`: Allocated during eval, or for string tensors.
/// * `kTfLitePersistentRo`: Allocated and populated during prepare. This is
/// useful for tensors that can be computed during prepare and treated
/// as constant inputs for downstream ops (also in prepare).
/// * `kTfLiteCustom`: Custom memory allocation provided by the user. See
/// TfLiteCustomAllocation below.
/// * `kTfLiteVariantObject`: Allocation is an arbitrary type-erased C++
/// object.
/// Allocation and deallocation are done through `new` and `delete`.
/// * `kTfLiteNonCpu`: Tensor buffer is in non-CPU memory, such as AHWB, GPU
/// memory. This tensor is not accessed by the CPU.
/// This is only used by LiteRt API.
enum TfLiteAllocationType {
  kTfLiteMemNone(0),
  kTfLiteMmapRo(1),
  kTfLiteArenaRw(2),
  kTfLiteArenaRwPersistent(3),
  kTfLiteDynamic(4),
  kTfLitePersistentRo(5),
  kTfLiteCustom(6),
  kTfLiteVariantObject(7),
  kTfLiteNonCpu(8);

  final int value;
  const TfLiteAllocationType(this.value);

  static TfLiteAllocationType fromValue(int value) => switch (value) {
        0 => kTfLiteMemNone,
        1 => kTfLiteMmapRo,
        2 => kTfLiteArenaRw,
        3 => kTfLiteArenaRwPersistent,
        4 => kTfLiteDynamic,
        5 => kTfLitePersistentRo,
        6 => kTfLiteCustom,
        7 => kTfLiteVariantObject,
        8 => kTfLiteNonCpu,
        _ =>
          throw ArgumentError("Unknown value for TfLiteAllocationType: $value"),
      };
}

/// The delegates should use zero or positive integers to represent handles.
/// -1 is reserved from unallocated status.
typedef TfLiteBufferHandle = ffi.Int;
typedef DartTfLiteBufferHandle = int;

/// Structure specifying the quantization used by the tensor, if-any.
final class TfLiteQuantization extends ffi.Struct {
  /// The type of quantization held by params.
  @ffi.UnsignedInt()
  external int type;

  /// Holds an optional reference to a quantization param structure. The actual
  /// type depends on the value of the `type` field (see the comment there for
  /// the values and corresponding types).
  external ffi.Pointer<ffi.Void> params;
}

enum TfLiteQuantizationType {
  /// No quantization.
  kTfLiteNoQuantization(0),

  /// Affine quantization (with support for per-channel quantization).
  /// Corresponds to TfLiteAffineQuantization.
  kTfLiteAffineQuantization(1),

  /// Blockwise quantization.
  kTfLiteBlockwiseQuantization(2);

  final int value;
  const TfLiteQuantizationType(this.value);

  static TfLiteQuantizationType fromValue(int value) => switch (value) {
        0 => kTfLiteNoQuantization,
        1 => kTfLiteAffineQuantization,
        2 => kTfLiteBlockwiseQuantization,
        _ => throw ArgumentError(
            "Unknown value for TfLiteQuantizationType: $value"),
      };
}

/// Parameters used to encode a sparse tensor. For detailed explanation of each
/// field please refer to lite/schema/schema.fbs.
final class TfLiteSparsity extends ffi.Struct {
  external ffi.Pointer<TfLiteIntArray> traversal_order;

  external ffi.Pointer<TfLiteIntArray> block_map;

  external ffi.Pointer<TfLiteDimensionMetadata> dim_metadata;

  @ffi.Int()
  external int dim_metadata_size;
}

/// Metadata to encode each dimension in a sparse tensor.
final class TfLiteDimensionMetadata extends ffi.Struct {
  @ffi.UnsignedInt()
  external int format;

  @ffi.Int()
  external int dense_size;

  external ffi.Pointer<TfLiteIntArray> array_segments;

  external ffi.Pointer<TfLiteIntArray> array_indices;
}

/// A structure representing an instance of a node.
/// This structure only exhibits the inputs, outputs, user defined data and some
/// node properties (like statefulness), not other features like the type.
final class TfLiteNode extends ffi.Struct {
  /// Inputs to this node expressed as indices into the simulator's tensors.
  external ffi.Pointer<TfLiteIntArray> inputs;

  /// Outputs to this node expressed as indices into the simulator's tensors.
  external ffi.Pointer<TfLiteIntArray> outputs;

  /// intermediate tensors to this node expressed as indices into the
  /// simulator's tensors.
  external ffi.Pointer<TfLiteIntArray> intermediates;

  /// Temporary tensors uses during the computations. This usually contains no
  /// tensors, but ops are allowed to change that if they need scratch space of
  /// any sort.
  external ffi.Pointer<TfLiteIntArray> temporaries;

  /// Opaque data provided by the node implementer through `Registration.init`.
  external ffi.Pointer<ffi.Void> user_data;

  /// Opaque data provided to the node if the node is a builtin. This is usually
  /// a structure defined in builtin_op_data.h
  external ffi.Pointer<ffi.Void> builtin_data;

  /// Custom initial data. This is the opaque data provided in the flatbuffer.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<ffi.Void> custom_initial_data;

  @ffi.Int()
  external int custom_initial_data_size;

  /// The pointer to the delegate. This is non-null only when the node is
  /// created by calling `interpreter.ModifyGraphWithDelegate`.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<TfLiteDelegate> delegate;

  /// Whether this op might have side effect (e.g. stateful op).
  @ffi.Bool()
  external bool might_have_side_effect;
}

/// `TfLiteRegistration` defines the implementation of an operation
/// (a built-in op, custom op, or custom delegate kernel).
///
/// It is a struct containing "methods" (C function pointers) that will be
/// invoked by the TF Lite runtime to evaluate instances of the operation.
///
/// See also `TfLiteOperator` which is a more ABI-stable equivalent.
final class TfLiteRegistration extends ffi.Struct {
  /// Initializes the op from serialized data.
  /// Called only *once* for the lifetime of the op, so any one-time allocations
  /// should be made here (unless they depend on tensor sizes).
  ///
  /// * If a built-in op:
  /// * `buffer` is the op's params data (TfLiteLSTMParams*).
  /// * `length` is zero.
  /// * If custom op:
  /// * `buffer` is the op's `custom_options`.
  /// * `length` is the size of the buffer.
  ///
  /// Returns a type-punned (i.e. void*) opaque data (e.g. a primitive pointer
  /// or an instance of a struct).
  ///
  /// The returned pointer will be stored with the node in the `user_data`
  /// field, accessible within prepare and invoke functions below.
  ///
  /// NOTE: if the data is already in the desired format, simply implement this
  /// function to return `nullptr` and implement the free function to be a
  /// no-op.
  ///
  /// NOTE: For a Delegate kernel, returns `TfLiteKernelInitFailed()` if it
  /// fails on the initialization. This eventually causes user's API call to
  /// InterpreterBuilder::operator() or Interpreter::ModifyGraphWithDelegate()
  /// to return an error.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  /// The pointer `buffer` is the data previously returned by an init
  /// invocation.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  /// prepare is called when the inputs this node depends on have been resized.
  /// `context->ResizeTensor()` can be called to request output tensors to be
  /// resized.
  /// Can be called multiple times for the lifetime of the op.
  ///
  /// Returns `kTfLiteOk` on success.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  /// Execute the node (should read `node->inputs` and output to
  /// `node->outputs`).
  ///
  /// Returns `kTfLiteOk` on success.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  /// `profiling_string` is called during summarization of profiling information
  /// in order to group executions together. Providing a value here will cause a
  /// given op to appear multiple times is the profiling report. This is
  /// particularly useful for custom ops that can perform significantly
  /// different calculations depending on their `user-data`.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  /// Builtin codes. If this kernel refers to a builtin this is the code
  /// of the builtin. This is so we can do marshaling to other frameworks like
  /// NN API.
  ///
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  @ffi.Int32()
  external int builtin_code;

  /// Custom op name. If the op is a builtin, this will be `null`.
  ///
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  ///
  /// WARNING: This is an experimental interface that is subject to change.
  external ffi.Pointer<ffi.Char> custom_name;

  /// The version of the op.
  /// Note: It is the responsibility of the registration binder to set this
  /// properly.
  @ffi.Int()
  external int version;

  /// The external (i.e. ABI-stable) version of `TfLiteRegistration`.
  /// Since we can't use internal types (such as `TfLiteContext`) for C API to
  /// maintain ABI stability.  C API user will provide `TfLiteOperator` to
  /// implement custom ops.  We keep it inside of `TfLiteRegistration` and use
  /// it to route callbacks properly.
  external ffi.Pointer<TfLiteOperator> registration_external;

  /// Retrieves asynchronous kernel.
  ///
  /// If the `async_kernel` field is nullptr, it means the operation described
  /// by this TfLiteRegistration object does not support asynchronous execution.
  /// Otherwise, the function that the field points to should only be called for
  /// delegate kernel nodes, i.e. `node` should be a delegate kernel node
  /// created by applying a delegate. If the function returns nullptr, that
  /// means that the underlying delegate does not support asynchronous execution
  /// for this `node`.
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteAsyncKernel> Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> async_kernel;

  /// Indicates if an operator's output may safely overwrite its inputs.
  /// See the comments in `TfLiteInPlaceOp`.
  @ffi.Uint64()
  external int inplace_operator;
}

final class TfLiteOperator extends ffi.Opaque {}

/// An external context is a collection of information unrelated to the TF Lite
/// framework, but useful to a subset of the ops. TF Lite knows very little
/// about the actual contexts, but it keeps a list of them, and is able to
/// refresh them if configurations like the number of recommended threads
/// change.
final class TfLiteExternalContext extends ffi.Struct {
  @ffi.UnsignedInt()
  external int type;

  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context)>>
      Refresh;
}

/// The list of external context types known to TF Lite. This list exists solely
/// to avoid conflicts and to ensure ops can share the external contexts they
/// need. Access to the external contexts is controlled by one of the
/// corresponding support files.
enum TfLiteExternalContextType {
  kTfLiteEigenContext(0),

  /// include eigen_support.h to use.
  kTfLiteGemmLowpContext(1),

  /// include gemm_support.h to use.
  kTfLiteEdgeTpuContext(2),

  /// Placeholder for Edge TPU support.
  kTfLiteCpuBackendContext(3),

  /// include cpu_backend_context.h to use.
  kTfLiteLiteRtBufferContext(4),

  /// include external_litert_buffer_context.h to use.
  kTfLiteMaxExternalContexts(5);

  final int value;
  const TfLiteExternalContextType(this.value);

  static TfLiteExternalContextType fromValue(int value) => switch (value) {
        0 => kTfLiteEigenContext,
        1 => kTfLiteGemmLowpContext,
        2 => kTfLiteEdgeTpuContext,
        3 => kTfLiteCpuBackendContext,
        4 => kTfLiteLiteRtBufferContext,
        5 => kTfLiteMaxExternalContexts,
        _ => throw ArgumentError(
            "Unknown value for TfLiteExternalContextType: $value"),
      };
}

/// WARNING: This is an experimental interface that is subject to change.
///
/// Currently, TfLiteDelegateParams has to be allocated in a way that it's
/// trivially destructable. It will be stored as `builtin_data` field in
/// `TfLiteNode` of the delegate node.
///
/// See also the `CreateDelegateParams` function in `interpreter.cc` details.
final class TfLiteDelegateParams extends ffi.Struct {
  external ffi.Pointer<TfLiteDelegate> delegate;

  external ffi.Pointer<TfLiteIntArray> nodes_to_replace;

  external ffi.Pointer<TfLiteIntArray> input_tensors;

  external ffi.Pointer<TfLiteIntArray> output_tensors;
}

/// Light-weight tensor struct for TF Micro runtime. Provides the minimal amount
/// of information required for a kernel to run during TfLiteRegistration::Eval.
/// TODO(b/160955687): Move this field into TF_LITE_STATIC_MEMORY when TFLM
/// builds with this flag by default internally.
final class TfLiteEvalTensor extends ffi.Struct {
  /// A union of data pointers. The appropriate type should be used for a typed
  /// tensor based on `type`.
  external TfLitePtrUnion data;

  /// A pointer to a structure representing the dimensionality interpretation
  /// that the buffer should have.
  external ffi.Pointer<TfLiteIntArray> dims;

  /// The data type specification for data stored in `data`. This affects
  /// what member of `data` union should be used.
  @ffi.UnsignedInt()
  external int type;
}

/// `TfLiteOpaqueDelegateBuilder` is used for constructing
/// `TfLiteOpaqueDelegate`, see `TfLiteOpaqueDelegateCreate` in c_api_opaque.h.
/// NOTE: This struct is not ABI stable.
///
/// For forward source compatibility `TfLiteOpaqueDelegateBuilder` objects
/// should be brace-initialized, so that all fields (including any that might be
/// added in the future) get zero-initialized.  The purpose of each field is
/// exactly the same as with `TfLiteDelegate`.
///
/// NOTE: This type is part of the TensorFlow Lite Extension APIs.
/// We reserve the right to make changes to this API in future releases,
/// potentially including non-backwards-compatible changes, on a different
/// schedule than for the other TensorFlow Lite APIs. See
/// https://www.tensorflow.org/guide/versions#separate_version_number_for_tensorflow_lite_extension_apis.
final class TfLiteOpaqueDelegateBuilder extends ffi.Struct {
  /// Data that delegate needs to identify itself. This data is owned by the
  /// delegate. The delegate is owned in the user code, so the delegate is
  /// responsible for deallocating this when it is destroyed.
  external ffi.Pointer<ffi.Void> data;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegate> delegate,
              ffi.Pointer<ffi.Void> data)>> Prepare;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegate> delegate,
              ffi.Pointer<ffi.Void> data,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteOpaqueTensor> tensor)>> CopyFromBufferHandle;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegate> delegate,
              ffi.Pointer<ffi.Void> data,
              TfLiteBufferHandle buffer_handle,
              ffi.Pointer<TfLiteOpaqueTensor> tensor)>> CopyToBufferHandle;

  /// NOLINT
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<TfLiteOpaqueContext> context,
              ffi.Pointer<TfLiteOpaqueDelegate> delegate,
              ffi.Pointer<ffi.Void> data,
              ffi.Pointer<TfLiteBufferHandle> handle)>> FreeBufferHandle;

  /// Bitmask flags. See the comments in `TfLiteDelegateFlags`.
  @ffi.Int64()
  external int flags;
}

typedef TfLiteOpaqueDelegate = TfLiteDelegate;

final class TfLiteOpaqueDelegateStruct extends ffi.Opaque {}

final class TfLiteModel extends ffi.Opaque {}

final class TfLiteInterpreterOptions extends ffi.Opaque {}

final class TfLiteInterpreter extends ffi.Opaque {}

final class TfLiteSignatureRunner extends ffi.Opaque {}

/// Fixed size list of floats. Used for per-channel quantization.
final class TfLiteFloatArray extends ffi.Opaque {}

/// Parameters for asymmetric quantization across a dimension (i.e per output
/// channel quantization).
/// quantized_dimension specifies which dimension the scales and zero_points
/// correspond to.
/// For a particular value in quantized_dimension, quantized values can be
/// converted back to float using:
/// `real_value = scale * (quantized_value - zero_point)`
final class TfLiteAffineQuantization extends ffi.Struct {
  external ffi.Pointer<TfLiteFloatArray> scale;

  external ffi.Pointer<TfLiteIntArray> zero_point;

  @ffi.Int32()
  external int quantized_dimension;
}

/// Parameters for blockwise quantization across the output channels dimension.
/// For a particular value in quantized_dimension, quantized values can be
/// converted back to float using:
/// `real_value = scale * (quantized_value - zero_point)`
final class TfLiteBlockwiseQuantization extends ffi.Struct {
  /// Index of the tensor containing the scales.
  @ffi.Int32()
  external int scale;

  /// Index of the tensor containing the zero points.
  @ffi.Int32()
  external int zero_point;

  /// Quantization blocksize.
  @ffi.Int32()
  external int blocksize;

  @ffi.Int32()
  external int quantized_dimension;
}

/// Memory allocation strategies.
///
/// TfLiteAllocationType values have been overloaded to mean more than their
/// original intent. This enum should only be used to document the allocation
/// strategy used by a tensor for it data.
enum TfLiteAllocationStrategy {
  kTfLiteAllocationStrategyUnknown(0),
  kTfLiteAllocationStrategyNone(1),

  /// No data is allocated.
  kTfLiteAllocationStrategyMMap(2),

  /// Data is mmaped.
  kTfLiteAllocationStrategyArena(3),

  /// Handled by the arena.
  kTfLiteAllocationStrategyMalloc(4),

  /// Uses `malloc`/`free`.
  kTfLiteAllocationStrategyNew(5);

  final int value;
  const TfLiteAllocationStrategy(this.value);

  static TfLiteAllocationStrategy fromValue(int value) => switch (value) {
        0 => kTfLiteAllocationStrategyUnknown,
        1 => kTfLiteAllocationStrategyNone,
        2 => kTfLiteAllocationStrategyMMap,
        3 => kTfLiteAllocationStrategyArena,
        4 => kTfLiteAllocationStrategyMalloc,
        5 => kTfLiteAllocationStrategyNew,
        _ => throw ArgumentError(
            "Unknown value for TfLiteAllocationStrategy: $value"),
      };
}

/// Describes how stable a tensor attribute is with regards to an interpreter
/// runs.
enum TfLiteRunStability {
  kTfLiteRunStabilityUnknown(0),
  kTfLiteRunStabilityUnstable(1),

  /// May change at any time.
  kTfLiteRunStabilitySingleRun(2),

  /// Will stay the same for one run.
  kTfLiteRunStabilityAcrossRuns(3);

  final int value;
  const TfLiteRunStability(this.value);

  static TfLiteRunStability fromValue(int value) => switch (value) {
        0 => kTfLiteRunStabilityUnknown,
        1 => kTfLiteRunStabilityUnstable,
        2 => kTfLiteRunStabilitySingleRun,
        3 => kTfLiteRunStabilityAcrossRuns,
        _ =>
          throw ArgumentError("Unknown value for TfLiteRunStability: $value"),
      };
}

/// Describes the steps of a TFLite operation life cycle.
enum TfLiteRunStep {
  kTfLiteRunStepUnknown(0),
  kTfLiteRunStepInit(1),
  kTfLiteRunStepPrepare(2),
  kTfLiteRunStepEval(3);

  final int value;
  const TfLiteRunStep(this.value);

  static TfLiteRunStep fromValue(int value) => switch (value) {
        0 => kTfLiteRunStepUnknown,
        1 => kTfLiteRunStepInit,
        2 => kTfLiteRunStepPrepare,
        3 => kTfLiteRunStepEval,
        _ => throw ArgumentError("Unknown value for TfLiteRunStep: $value"),
      };
}

/// Defines a custom memory allocation not owned by the runtime.
/// `data` should be aligned to kDefaultTensorAlignment defined in
/// lite/util.h. (Currently 64 bytes)
/// NOTE: See `Interpreter::SetCustomAllocationForTensor` for details on usage.
final class TfLiteCustomAllocation extends ffi.Struct {
  external ffi.Pointer<ffi.Void> data;

  @ffi.Size()
  external int bytes;
}

/// The flags used in `Interpreter::SetCustomAllocationForTensor`.
/// Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc.
enum TfLiteCustomAllocationFlags {
  kTfLiteCustomAllocationFlagsNone(0),

  /// Skips checking whether allocation.data points to an aligned buffer as
  /// expected by the TFLite runtime.
  /// NOTE: Setting this flag can cause crashes when calling Invoke().
  /// Use with caution.
  kTfLiteCustomAllocationFlagsSkipAlignCheck(1);

  final int value;
  const TfLiteCustomAllocationFlags(this.value);

  static TfLiteCustomAllocationFlags fromValue(int value) => switch (value) {
        0 => kTfLiteCustomAllocationFlagsNone,
        1 => kTfLiteCustomAllocationFlagsSkipAlignCheck,
        _ => throw ArgumentError(
            "Unknown value for TfLiteCustomAllocationFlags: $value"),
      };
}

/// WARNING: This is an experimental interface that is subject to change.
///
/// Currently, TfLiteOpaqueDelegateParams has to be allocated in a way that it's
/// trivially destructable. It will be stored as `builtin_data` field in
/// `TfLiteNode` of the delegate node.
///
/// See also the `CreateOpaqueDelegateParams` function in `subgraph.cc`
/// details.
final class TfLiteOpaqueDelegateParams extends ffi.Struct {
  external ffi.Pointer<TfLiteOpaqueDelegate> delegate;

  external ffi.Pointer<ffi.Void> delegate_data;

  external ffi.Pointer<TfLiteIntArray> nodes_to_replace;

  external ffi.Pointer<TfLiteIntArray> input_tensors;

  external ffi.Pointer<TfLiteIntArray> output_tensors;
}

/// The valid values of the `inplace_operator` field in `TfLiteRegistration`.
/// This allow an op to signal to the runtime that the same data pointer
/// may be passed as an input and output without impacting the result.
/// This does not mean that the memory can safely be reused, it is up to the
/// runtime to determine this, e.g. if another op consumes the same input or not
/// or if an input tensor has sufficient memory allocated to store the output
/// data.
///
/// Setting these flags authorizes the runtime to set the data pointers of an
/// input and output tensor to the same value. In such cases, the memory
/// required by the output must be less than or equal to that required by the
/// shared input, never greater. If kTfLiteInplaceOpDataUnmodified is set, then
/// the runtime can share the same input tensor with multiple operator's
/// outputs, provided that kTfLiteInplaceOpDataUnmodified is set for all of
/// them. Otherwise, if an input tensor is consumed by multiple operators, it
/// may only be shared with the operator which is the last to consume it.
///
/// Note that this is a bitmask, so the values should be 1, 2, 4, 8, ...etc.
enum TfLiteInPlaceOp {
  /// The default value. This indicates that the same data pointer cannot safely
  /// be passed as an op's input and output.
  kTfLiteInplaceOpNone(0),

  /// This indicates that an op's first output's data is identical to its first
  /// input's data, for example Reshape.
  kTfLiteInplaceOpDataUnmodified(1),

  /// Setting kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput means
  /// that InputN may be shared with OutputN instead of with the first output.
  /// This flag requires one or more of kTfLiteInplaceOpInputNShared to be set.
  kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput(2),

  /// kTfLiteInplaceOpInputNShared indicates that it is safe for an op to share
  /// InputN's data pointer with an output tensor. If
  /// kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is set then
  /// kTfLiteInplaceOpInputNShared indicates that InputN may be shared
  /// with OutputN, otherwise kTfLiteInplaceOpInputNShared indicates that InputN
  /// may be shared with the first output.
  ///
  /// Indicates that an op's first input may be shared with the first output
  /// tensor. kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput has
  /// no impact on the behavior allowed by this flag.
  kTfLiteInplaceOpInput0Shared(4),

  /// Indicates that an op's second input may be shared with the first output
  /// if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
  /// or second output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput
  /// is set.
  kTfLiteInplaceOpInput1Shared(8),

  /// Indicates that an op's third input may be shared with the first output
  /// if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput is not set
  /// or third output if kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput
  /// is
  /// set.
  kTfLiteInplaceOpInput2Shared(16),

  /// Placeholder to ensure that enum can hold 64 bit values to accommodate
  /// future fields.
  kTfLiteInplaceOpMaxValue(-1);

  final int value;
  const TfLiteInPlaceOp(this.value);

  static TfLiteInPlaceOp fromValue(int value) => switch (value) {
        0 => kTfLiteInplaceOpNone,
        1 => kTfLiteInplaceOpDataUnmodified,
        2 => kTfLiteInplaceInputCanBeSharedWithCorrespondingOutput,
        4 => kTfLiteInplaceOpInput0Shared,
        8 => kTfLiteInplaceOpInput1Shared,
        16 => kTfLiteInplaceOpInput2Shared,
        -1 => kTfLiteInplaceOpMaxValue,
        _ => throw ArgumentError("Unknown value for TfLiteInPlaceOp: $value"),
      };
}

/// \private
/// Old version of `TfLiteRegistration` to maintain binary backward
/// compatibility.
/// The legacy registration type must be a POD struct type whose field types
/// must be a prefix of the field types in TfLiteRegistration, and offset of the
/// first field in TfLiteRegistration that is not present in the legacy
/// registration type must be greater than or equal to the size of the legacy
/// registration type.
///
/// WARNING: This structure is deprecated / not an official part of the
/// API. It should be only used for binary backward compatibility.
final class TfLiteRegistration_V3 extends ffi.Struct {
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  @ffi.Int32()
  external int builtin_code;

  external ffi.Pointer<ffi.Char> custom_name;

  @ffi.Int()
  external int version;

  external ffi.Pointer<TfLiteOperator> registration_external;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<TfLiteAsyncKernel> Function(
              ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> async_kernel;
}

/// \private
/// Old version of `TfLiteRegistration` to maintain binary backward
/// compatibility.
/// The legacy registration type must be a POD struct type whose field types
/// must be a prefix of the field types in TfLiteRegistration, and offset of the
/// first field in TfLiteRegistration that is not present in the legacy
/// registration type must be greater than or equal to the size of the legacy
/// registration type.
///
/// WARNING: This structure is deprecated / not an official part of the
/// API. It should be only used for binary backward compatibility.
final class TfLiteRegistration_V2 extends ffi.Struct {
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  @ffi.Int32()
  external int builtin_code;

  external ffi.Pointer<ffi.Char> custom_name;

  @ffi.Int()
  external int version;

  external ffi.Pointer<TfLiteOperator> registration_external;
}

/// \private
/// Old version of `TfLiteRegistration` to maintain binary backward
/// compatibility.
/// The legacy registration type must be a POD struct type whose field types
/// must be a prefix of the field types in TfLiteRegistration, and offset of the
/// first field in TfLiteRegistration that is not present in the legacy
/// registration type must be greater than or equal to the size of the legacy
/// registration type.
///
/// WARNING: This structure is deprecated / not an official part of the
/// API. It should be only used for binary backward compatibility.
final class TfLiteRegistration_V1 extends ffi.Struct {
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Char> buffer, ffi.Size length)>> init;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<ffi.Void> buffer)>> free;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> prepare;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.UnsignedInt Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> invoke;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<TfLiteContext> context,
              ffi.Pointer<TfLiteNode> node)>> profiling_string;

  @ffi.Int32()
  external int builtin_code;

  external ffi.Pointer<ffi.Char> custom_name;

  @ffi.Int()
  external int version;
}

/// The flags used in `TfLiteDelegate`. Note that this is a bitmask, so the
/// values should be 1, 2, 4, 8, ...etc.
enum TfLiteDelegateFlags {
  kTfLiteDelegateFlagsNone(0),

  /// The flag is set if the delegate can handle dynamic sized tensors.
  /// For example, the output shape of a `Resize` op with non-constant shape
  /// can only be inferred when the op is invoked.
  /// In this case, the Delegate is responsible for calling
  /// `SetTensorToDynamic` to mark the tensor as a dynamic tensor, and calling
  /// `ResizeTensor` when invoking the op.
  ///
  /// If the delegate isn't capable to handle dynamic tensors, this flag need
  /// to be set to false.
  kTfLiteDelegateFlagsAllowDynamicTensors(1),

  /// This flag can be used by delegates (that allow dynamic tensors) to ensure
  /// applicable tensor shapes are automatically propagated in the case of
  /// tensor resizing. This means that non-dynamic (allocation_type !=
  /// kTfLiteDynamic) I/O tensors of a delegate kernel will have correct shapes
  /// before its Prepare() method is called. The runtime leverages TFLite
  /// builtin ops in the original execution plan to propagate shapes.
  ///
  /// A few points to note:
  /// 1. This requires kTfLiteDelegateFlagsAllowDynamicTensors. If that flag is
  /// false, this one is redundant since the delegate kernels are re-initialized
  /// every time tensors are resized.
  /// 2. Enabling this flag adds some overhead to AllocateTensors(), since extra
  /// work is required to prepare the original execution plan.
  /// 3. This flag requires that the original execution plan only have ops with
  /// valid registrations (and not 'dummy' custom ops like with Flex).
  ///
  /// WARNING: This feature is experimental and subject to change.
  kTfLiteDelegateFlagsRequirePropagatedShapes(2),

  /// This flag can be used by delegates to request per-operator profiling. If a
  /// node is a delegate node, this flag will be checked before profiling. If
  /// set, then the node will not be profiled. The delegate will then add per
  /// operator information using `Profiler::EventType::OPERATOR_INVOKE_EVENT`
  /// and the results will appear in the operator-wise Profiling section and not
  /// in the Delegate internal section.
  kTfLiteDelegateFlagsPerOperatorProfiling(4),

  /// This flag can be used by callers to hint that the delegate is likely to
  /// delegate the entire graph to a single delegate so certain allocations can
  /// be skipped.
  /// This is an ADVANCED feature and should only be used if the caller has
  /// prior knowledge that the delegate will fully delegate all subgraphs
  /// to a single delegate.
  kTfLiteDelegateFlagsHintFullyDelegatedToSingleDelegate(8);

  final int value;
  const TfLiteDelegateFlags(this.value);

  static TfLiteDelegateFlags fromValue(int value) => switch (value) {
        0 => kTfLiteDelegateFlagsNone,
        1 => kTfLiteDelegateFlagsAllowDynamicTensors,
        2 => kTfLiteDelegateFlagsRequirePropagatedShapes,
        4 => kTfLiteDelegateFlagsPerOperatorProfiling,
        8 => kTfLiteDelegateFlagsHintFullyDelegatedToSingleDelegate,
        _ =>
          throw ArgumentError("Unknown value for TfLiteDelegateFlags: $value"),
      };
}

const int kTfLiteNullBufferHandle = -1;

const int kTfLiteNoBufferIdentifier = -1;

const int __bool_true_false_are_defined = 1;

const int true1 = 1;

const int false1 = 0;

const int __WORDSIZE = 64;

const int __has_safe_buffers = 1;

const int __DARWIN_ONLY_64_BIT_INO_T = 1;

const int __DARWIN_ONLY_UNIX_CONFORMANCE = 1;

const int __DARWIN_ONLY_VERS_1050 = 1;

const int __DARWIN_UNIX03 = 1;

const int __DARWIN_64_BIT_INO_T = 1;

const int __DARWIN_VERS_1050 = 1;

const int __DARWIN_NON_CANCELABLE = 0;

const String __DARWIN_SUF_EXTSN = '\$DARWIN_EXTSN';

const int __DARWIN_C_ANSI = 4096;

const int __DARWIN_C_FULL = 900000;

const int __DARWIN_C_LEVEL = 900000;

const int __STDC_WANT_LIB_EXT1__ = 1;

const int __DARWIN_NO_LONG_LONG = 0;

const int _DARWIN_FEATURE_64_BIT_INODE = 1;

const int _DARWIN_FEATURE_ONLY_64_BIT_INODE = 1;

const int _DARWIN_FEATURE_ONLY_VERS_1050 = 1;

const int _DARWIN_FEATURE_ONLY_UNIX_CONFORMANCE = 1;

const int _DARWIN_FEATURE_UNIX_CONFORMANCE = 3;

const int __has_ptrcheck = 0;

const int __has_bounds_safety_attributes = 0;

const int __DARWIN_NULL = 0;

const int __PTHREAD_SIZE__ = 8176;

const int __PTHREAD_ATTR_SIZE__ = 56;

const int __PTHREAD_MUTEXATTR_SIZE__ = 8;

const int __PTHREAD_MUTEX_SIZE__ = 56;

const int __PTHREAD_CONDATTR_SIZE__ = 8;

const int __PTHREAD_COND_SIZE__ = 40;

const int __PTHREAD_ONCE_SIZE__ = 8;

const int __PTHREAD_RWLOCK_SIZE__ = 192;

const int __PTHREAD_RWLOCKATTR_SIZE__ = 16;

const int INT8_MAX = 127;

const int INT16_MAX = 32767;

const int INT32_MAX = 2147483647;

const int INT64_MAX = 9223372036854775807;

const int INT8_MIN = -128;

const int INT16_MIN = -32768;

const int INT32_MIN = -2147483648;

const int INT64_MIN = -9223372036854775808;

const int UINT8_MAX = 255;

const int UINT16_MAX = 65535;

const int UINT32_MAX = 4294967295;

const int UINT64_MAX = -1;

const int INT_LEAST8_MIN = -128;

const int INT_LEAST16_MIN = -32768;

const int INT_LEAST32_MIN = -2147483648;

const int INT_LEAST64_MIN = -9223372036854775808;

const int INT_LEAST8_MAX = 127;

const int INT_LEAST16_MAX = 32767;

const int INT_LEAST32_MAX = 2147483647;

const int INT_LEAST64_MAX = 9223372036854775807;

const int UINT_LEAST8_MAX = 255;

const int UINT_LEAST16_MAX = 65535;

const int UINT_LEAST32_MAX = 4294967295;

const int UINT_LEAST64_MAX = -1;

const int INT_FAST8_MIN = -128;

const int INT_FAST16_MIN = -32768;

const int INT_FAST32_MIN = -2147483648;

const int INT_FAST64_MIN = -9223372036854775808;

const int INT_FAST8_MAX = 127;

const int INT_FAST16_MAX = 32767;

const int INT_FAST32_MAX = 2147483647;

const int INT_FAST64_MAX = 9223372036854775807;

const int UINT_FAST8_MAX = 255;

const int UINT_FAST16_MAX = 65535;

const int UINT_FAST32_MAX = 4294967295;

const int UINT_FAST64_MAX = -1;

const int INTPTR_MAX = 9223372036854775807;

const int INTPTR_MIN = -9223372036854775808;

const int UINTPTR_MAX = -1;

const int INTMAX_MAX = 9223372036854775807;

const int UINTMAX_MAX = -1;

const int INTMAX_MIN = -9223372036854775808;

const int PTRDIFF_MIN = -9223372036854775808;

const int PTRDIFF_MAX = 9223372036854775807;

const int SIZE_MAX = -1;

const int RSIZE_MAX = 9223372036854775807;

const int WCHAR_MAX = 2147483647;

const int WCHAR_MIN = -2147483648;

const int WINT_MIN = -2147483648;

const int WINT_MAX = 2147483647;

const int SIG_ATOMIC_MIN = -2147483648;

const int SIG_ATOMIC_MAX = 2147483647;

const int __API_TO_BE_DEPRECATED = 100000;

const int __API_TO_BE_DEPRECATED_MACOS = 100000;

const int __API_TO_BE_DEPRECATED_MACOSAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_IOS = 100000;

const int __API_TO_BE_DEPRECATED_IOSAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_MACCATALYST = 100000;

const int __API_TO_BE_DEPRECATED_MACCATALYSTAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_WATCHOS = 100000;

const int __API_TO_BE_DEPRECATED_WATCHOSAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_TVOS = 100000;

const int __API_TO_BE_DEPRECATED_TVOSAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_DRIVERKIT = 100000;

const int __API_TO_BE_DEPRECATED_VISIONOS = 100000;

const int __API_TO_BE_DEPRECATED_VISIONOSAPPLICATIONEXTENSION = 100000;

const int __API_TO_BE_DEPRECATED_KERNELKIT = 100000;

const int __MAC_10_0 = 1000;

const int __MAC_10_1 = 1010;

const int __MAC_10_2 = 1020;

const int __MAC_10_3 = 1030;

const int __MAC_10_4 = 1040;

const int __MAC_10_5 = 1050;

const int __MAC_10_6 = 1060;

const int __MAC_10_7 = 1070;

const int __MAC_10_8 = 1080;

const int __MAC_10_9 = 1090;

const int __MAC_10_10 = 101000;

const int __MAC_10_10_2 = 101002;

const int __MAC_10_10_3 = 101003;

const int __MAC_10_11 = 101100;

const int __MAC_10_11_2 = 101102;

const int __MAC_10_11_3 = 101103;

const int __MAC_10_11_4 = 101104;

const int __MAC_10_12 = 101200;

const int __MAC_10_12_1 = 101201;

const int __MAC_10_12_2 = 101202;

const int __MAC_10_12_4 = 101204;

const int __MAC_10_13 = 101300;

const int __MAC_10_13_1 = 101301;

const int __MAC_10_13_2 = 101302;

const int __MAC_10_13_4 = 101304;

const int __MAC_10_14 = 101400;

const int __MAC_10_14_1 = 101401;

const int __MAC_10_14_4 = 101404;

const int __MAC_10_14_5 = 101405;

const int __MAC_10_14_6 = 101406;

const int __MAC_10_15 = 101500;

const int __MAC_10_15_1 = 101501;

const int __MAC_10_15_4 = 101504;

const int __MAC_10_16 = 101600;

const int __MAC_11_0 = 110000;

const int __MAC_11_1 = 110100;

const int __MAC_11_3 = 110300;

const int __MAC_11_4 = 110400;

const int __MAC_11_5 = 110500;

const int __MAC_11_6 = 110600;

const int __MAC_12_0 = 120000;

const int __MAC_12_1 = 120100;

const int __MAC_12_2 = 120200;

const int __MAC_12_3 = 120300;

const int __MAC_12_4 = 120400;

const int __MAC_12_5 = 120500;

const int __MAC_12_6 = 120600;

const int __MAC_12_7 = 120700;

const int __MAC_13_0 = 130000;

const int __MAC_13_1 = 130100;

const int __MAC_13_2 = 130200;

const int __MAC_13_3 = 130300;

const int __MAC_13_4 = 130400;

const int __MAC_13_5 = 130500;

const int __MAC_13_6 = 130600;

const int __MAC_13_7 = 130700;

const int __MAC_14_0 = 140000;

const int __MAC_14_1 = 140100;

const int __MAC_14_2 = 140200;

const int __MAC_14_3 = 140300;

const int __MAC_14_4 = 140400;

const int __MAC_14_5 = 140500;

const int __MAC_14_6 = 140600;

const int __MAC_14_7 = 140700;

const int __MAC_15_0 = 150000;

const int __MAC_15_1 = 150100;

const int __MAC_15_2 = 150200;

const int __MAC_15_3 = 150300;

const int __MAC_15_4 = 150400;

const int __MAC_15_5 = 150500;

const int __MAC_15_6 = 150600;

const int __MAC_16_0 = 160000;

const int __MAC_26_0 = 260000;

const int __MAC_26_1 = 260100;

const int __IPHONE_2_0 = 20000;

const int __IPHONE_2_1 = 20100;

const int __IPHONE_2_2 = 20200;

const int __IPHONE_3_0 = 30000;

const int __IPHONE_3_1 = 30100;

const int __IPHONE_3_2 = 30200;

const int __IPHONE_4_0 = 40000;

const int __IPHONE_4_1 = 40100;

const int __IPHONE_4_2 = 40200;

const int __IPHONE_4_3 = 40300;

const int __IPHONE_5_0 = 50000;

const int __IPHONE_5_1 = 50100;

const int __IPHONE_6_0 = 60000;

const int __IPHONE_6_1 = 60100;

const int __IPHONE_7_0 = 70000;

const int __IPHONE_7_1 = 70100;

const int __IPHONE_8_0 = 80000;

const int __IPHONE_8_1 = 80100;

const int __IPHONE_8_2 = 80200;

const int __IPHONE_8_3 = 80300;

const int __IPHONE_8_4 = 80400;

const int __IPHONE_9_0 = 90000;

const int __IPHONE_9_1 = 90100;

const int __IPHONE_9_2 = 90200;

const int __IPHONE_9_3 = 90300;

const int __IPHONE_10_0 = 100000;

const int __IPHONE_10_1 = 100100;

const int __IPHONE_10_2 = 100200;

const int __IPHONE_10_3 = 100300;

const int __IPHONE_11_0 = 110000;

const int __IPHONE_11_1 = 110100;

const int __IPHONE_11_2 = 110200;

const int __IPHONE_11_3 = 110300;

const int __IPHONE_11_4 = 110400;

const int __IPHONE_12_0 = 120000;

const int __IPHONE_12_1 = 120100;

const int __IPHONE_12_2 = 120200;

const int __IPHONE_12_3 = 120300;

const int __IPHONE_12_4 = 120400;

const int __IPHONE_13_0 = 130000;

const int __IPHONE_13_1 = 130100;

const int __IPHONE_13_2 = 130200;

const int __IPHONE_13_3 = 130300;

const int __IPHONE_13_4 = 130400;

const int __IPHONE_13_5 = 130500;

const int __IPHONE_13_6 = 130600;

const int __IPHONE_13_7 = 130700;

const int __IPHONE_14_0 = 140000;

const int __IPHONE_14_1 = 140100;

const int __IPHONE_14_2 = 140200;

const int __IPHONE_14_3 = 140300;

const int __IPHONE_14_5 = 140500;

const int __IPHONE_14_6 = 140600;

const int __IPHONE_14_7 = 140700;

const int __IPHONE_14_8 = 140800;

const int __IPHONE_15_0 = 150000;

const int __IPHONE_15_1 = 150100;

const int __IPHONE_15_2 = 150200;

const int __IPHONE_15_3 = 150300;

const int __IPHONE_15_4 = 150400;

const int __IPHONE_15_5 = 150500;

const int __IPHONE_15_6 = 150600;

const int __IPHONE_15_7 = 150700;

const int __IPHONE_15_8 = 150800;

const int __IPHONE_16_0 = 160000;

const int __IPHONE_16_1 = 160100;

const int __IPHONE_16_2 = 160200;

const int __IPHONE_16_3 = 160300;

const int __IPHONE_16_4 = 160400;

const int __IPHONE_16_5 = 160500;

const int __IPHONE_16_6 = 160600;

const int __IPHONE_16_7 = 160700;

const int __IPHONE_17_0 = 170000;

const int __IPHONE_17_1 = 170100;

const int __IPHONE_17_2 = 170200;

const int __IPHONE_17_3 = 170300;

const int __IPHONE_17_4 = 170400;

const int __IPHONE_17_5 = 170500;

const int __IPHONE_17_6 = 170600;

const int __IPHONE_17_7 = 170700;

const int __IPHONE_18_0 = 180000;

const int __IPHONE_18_1 = 180100;

const int __IPHONE_18_2 = 180200;

const int __IPHONE_18_3 = 180300;

const int __IPHONE_18_4 = 180400;

const int __IPHONE_18_5 = 180500;

const int __IPHONE_18_6 = 180600;

const int __IPHONE_19_0 = 190000;

const int __IPHONE_26_0 = 260000;

const int __IPHONE_26_1 = 260100;

const int __WATCHOS_1_0 = 10000;

const int __WATCHOS_2_0 = 20000;

const int __WATCHOS_2_1 = 20100;

const int __WATCHOS_2_2 = 20200;

const int __WATCHOS_3_0 = 30000;

const int __WATCHOS_3_1 = 30100;

const int __WATCHOS_3_1_1 = 30101;

const int __WATCHOS_3_2 = 30200;

const int __WATCHOS_4_0 = 40000;

const int __WATCHOS_4_1 = 40100;

const int __WATCHOS_4_2 = 40200;

const int __WATCHOS_4_3 = 40300;

const int __WATCHOS_5_0 = 50000;

const int __WATCHOS_5_1 = 50100;

const int __WATCHOS_5_2 = 50200;

const int __WATCHOS_5_3 = 50300;

const int __WATCHOS_6_0 = 60000;

const int __WATCHOS_6_1 = 60100;

const int __WATCHOS_6_2 = 60200;

const int __WATCHOS_7_0 = 70000;

const int __WATCHOS_7_1 = 70100;

const int __WATCHOS_7_2 = 70200;

const int __WATCHOS_7_3 = 70300;

const int __WATCHOS_7_4 = 70400;

const int __WATCHOS_7_5 = 70500;

const int __WATCHOS_7_6 = 70600;

const int __WATCHOS_8_0 = 80000;

const int __WATCHOS_8_1 = 80100;

const int __WATCHOS_8_3 = 80300;

const int __WATCHOS_8_4 = 80400;

const int __WATCHOS_8_5 = 80500;

const int __WATCHOS_8_6 = 80600;

const int __WATCHOS_8_7 = 80700;

const int __WATCHOS_8_8 = 80800;

const int __WATCHOS_9_0 = 90000;

const int __WATCHOS_9_1 = 90100;

const int __WATCHOS_9_2 = 90200;

const int __WATCHOS_9_3 = 90300;

const int __WATCHOS_9_4 = 90400;

const int __WATCHOS_9_5 = 90500;

const int __WATCHOS_9_6 = 90600;

const int __WATCHOS_10_0 = 100000;

const int __WATCHOS_10_1 = 100100;

const int __WATCHOS_10_2 = 100200;

const int __WATCHOS_10_3 = 100300;

const int __WATCHOS_10_4 = 100400;

const int __WATCHOS_10_5 = 100500;

const int __WATCHOS_10_6 = 100600;

const int __WATCHOS_10_7 = 100700;

const int __WATCHOS_11_0 = 110000;

const int __WATCHOS_11_1 = 110100;

const int __WATCHOS_11_2 = 110200;

const int __WATCHOS_11_3 = 110300;

const int __WATCHOS_11_4 = 110400;

const int __WATCHOS_11_5 = 110500;

const int __WATCHOS_11_6 = 110600;

const int __WATCHOS_12_0 = 120000;

const int __WATCHOS_26_0 = 260000;

const int __WATCHOS_26_1 = 260100;

const int __TVOS_9_0 = 90000;

const int __TVOS_9_1 = 90100;

const int __TVOS_9_2 = 90200;

const int __TVOS_10_0 = 100000;

const int __TVOS_10_0_1 = 100001;

const int __TVOS_10_1 = 100100;

const int __TVOS_10_2 = 100200;

const int __TVOS_11_0 = 110000;

const int __TVOS_11_1 = 110100;

const int __TVOS_11_2 = 110200;

const int __TVOS_11_3 = 110300;

const int __TVOS_11_4 = 110400;

const int __TVOS_12_0 = 120000;

const int __TVOS_12_1 = 120100;

const int __TVOS_12_2 = 120200;

const int __TVOS_12_3 = 120300;

const int __TVOS_12_4 = 120400;

const int __TVOS_13_0 = 130000;

const int __TVOS_13_2 = 130200;

const int __TVOS_13_3 = 130300;

const int __TVOS_13_4 = 130400;

const int __TVOS_14_0 = 140000;

const int __TVOS_14_1 = 140100;

const int __TVOS_14_2 = 140200;

const int __TVOS_14_3 = 140300;

const int __TVOS_14_5 = 140500;

const int __TVOS_14_6 = 140600;

const int __TVOS_14_7 = 140700;

const int __TVOS_15_0 = 150000;

const int __TVOS_15_1 = 150100;

const int __TVOS_15_2 = 150200;

const int __TVOS_15_3 = 150300;

const int __TVOS_15_4 = 150400;

const int __TVOS_15_5 = 150500;

const int __TVOS_15_6 = 150600;

const int __TVOS_16_0 = 160000;

const int __TVOS_16_1 = 160100;

const int __TVOS_16_2 = 160200;

const int __TVOS_16_3 = 160300;

const int __TVOS_16_4 = 160400;

const int __TVOS_16_5 = 160500;

const int __TVOS_16_6 = 160600;

const int __TVOS_17_0 = 170000;

const int __TVOS_17_1 = 170100;

const int __TVOS_17_2 = 170200;

const int __TVOS_17_3 = 170300;

const int __TVOS_17_4 = 170400;

const int __TVOS_17_5 = 170500;

const int __TVOS_17_6 = 170600;

const int __TVOS_18_0 = 180000;

const int __TVOS_18_1 = 180100;

const int __TVOS_18_2 = 180200;

const int __TVOS_18_3 = 180300;

const int __TVOS_18_4 = 180400;

const int __TVOS_18_5 = 180500;

const int __TVOS_18_6 = 180600;

const int __TVOS_19_0 = 190000;

const int __TVOS_26_0 = 260000;

const int __TVOS_26_1 = 260100;

const int __BRIDGEOS_2_0 = 20000;

const int __BRIDGEOS_3_0 = 30000;

const int __BRIDGEOS_3_1 = 30100;

const int __BRIDGEOS_3_4 = 30400;

const int __BRIDGEOS_4_0 = 40000;

const int __BRIDGEOS_4_1 = 40100;

const int __BRIDGEOS_5_0 = 50000;

const int __BRIDGEOS_5_1 = 50100;

const int __BRIDGEOS_5_3 = 50300;

const int __BRIDGEOS_6_0 = 60000;

const int __BRIDGEOS_6_2 = 60200;

const int __BRIDGEOS_6_4 = 60400;

const int __BRIDGEOS_6_5 = 60500;

const int __BRIDGEOS_6_6 = 60600;

const int __BRIDGEOS_7_0 = 70000;

const int __BRIDGEOS_7_1 = 70100;

const int __BRIDGEOS_7_2 = 70200;

const int __BRIDGEOS_7_3 = 70300;

const int __BRIDGEOS_7_4 = 70400;

const int __BRIDGEOS_7_6 = 70600;

const int __BRIDGEOS_8_0 = 80000;

const int __BRIDGEOS_8_1 = 80100;

const int __BRIDGEOS_8_2 = 80200;

const int __BRIDGEOS_8_3 = 80300;

const int __BRIDGEOS_8_4 = 80400;

const int __BRIDGEOS_8_5 = 80500;

const int __BRIDGEOS_8_6 = 80600;

const int __BRIDGEOS_9_0 = 90000;

const int __BRIDGEOS_9_1 = 90100;

const int __BRIDGEOS_9_2 = 90200;

const int __BRIDGEOS_9_3 = 90300;

const int __BRIDGEOS_9_4 = 90400;

const int __BRIDGEOS_9_5 = 90500;

const int __BRIDGEOS_9_6 = 90600;

const int __BRIDGEOS_10_0 = 100000;

const int __BRIDGEOS_10_1 = 100100;

const int __DRIVERKIT_19_0 = 190000;

const int __DRIVERKIT_20_0 = 200000;

const int __DRIVERKIT_21_0 = 210000;

const int __DRIVERKIT_22_0 = 220000;

const int __DRIVERKIT_22_4 = 220400;

const int __DRIVERKIT_22_5 = 220500;

const int __DRIVERKIT_22_6 = 220600;

const int __DRIVERKIT_23_0 = 230000;

const int __DRIVERKIT_23_1 = 230100;

const int __DRIVERKIT_23_2 = 230200;

const int __DRIVERKIT_23_3 = 230300;

const int __DRIVERKIT_23_4 = 230400;

const int __DRIVERKIT_23_5 = 230500;

const int __DRIVERKIT_23_6 = 230600;

const int __DRIVERKIT_24_0 = 240000;

const int __DRIVERKIT_24_1 = 240100;

const int __DRIVERKIT_24_2 = 240200;

const int __DRIVERKIT_24_3 = 240300;

const int __DRIVERKIT_24_4 = 240400;

const int __DRIVERKIT_24_5 = 240500;

const int __DRIVERKIT_24_6 = 240600;

const int __DRIVERKIT_25_0 = 250000;

const int __DRIVERKIT_25_1 = 250100;

const int __VISIONOS_1_0 = 10000;

const int __VISIONOS_1_1 = 10100;

const int __VISIONOS_1_2 = 10200;

const int __VISIONOS_1_3 = 10300;

const int __VISIONOS_2_0 = 20000;

const int __VISIONOS_2_1 = 20100;

const int __VISIONOS_2_2 = 20200;

const int __VISIONOS_2_3 = 20300;

const int __VISIONOS_2_4 = 20400;

const int __VISIONOS_2_5 = 20500;

const int __VISIONOS_2_6 = 20600;

const int __VISIONOS_3_0 = 30000;

const int __VISIONOS_26_0 = 260000;

const int __VISIONOS_26_1 = 260100;

const int MAC_OS_X_VERSION_10_0 = 1000;

const int MAC_OS_X_VERSION_10_1 = 1010;

const int MAC_OS_X_VERSION_10_2 = 1020;

const int MAC_OS_X_VERSION_10_3 = 1030;

const int MAC_OS_X_VERSION_10_4 = 1040;

const int MAC_OS_X_VERSION_10_5 = 1050;

const int MAC_OS_X_VERSION_10_6 = 1060;

const int MAC_OS_X_VERSION_10_7 = 1070;

const int MAC_OS_X_VERSION_10_8 = 1080;

const int MAC_OS_X_VERSION_10_9 = 1090;

const int MAC_OS_X_VERSION_10_10 = 101000;

const int MAC_OS_X_VERSION_10_10_2 = 101002;

const int MAC_OS_X_VERSION_10_10_3 = 101003;

const int MAC_OS_X_VERSION_10_11 = 101100;

const int MAC_OS_X_VERSION_10_11_2 = 101102;

const int MAC_OS_X_VERSION_10_11_3 = 101103;

const int MAC_OS_X_VERSION_10_11_4 = 101104;

const int MAC_OS_X_VERSION_10_12 = 101200;

const int MAC_OS_X_VERSION_10_12_1 = 101201;

const int MAC_OS_X_VERSION_10_12_2 = 101202;

const int MAC_OS_X_VERSION_10_12_4 = 101204;

const int MAC_OS_X_VERSION_10_13 = 101300;

const int MAC_OS_X_VERSION_10_13_1 = 101301;

const int MAC_OS_X_VERSION_10_13_2 = 101302;

const int MAC_OS_X_VERSION_10_13_4 = 101304;

const int MAC_OS_X_VERSION_10_14 = 101400;

const int MAC_OS_X_VERSION_10_14_1 = 101401;

const int MAC_OS_X_VERSION_10_14_4 = 101404;

const int MAC_OS_X_VERSION_10_14_5 = 101405;

const int MAC_OS_X_VERSION_10_14_6 = 101406;

const int MAC_OS_X_VERSION_10_15 = 101500;

const int MAC_OS_X_VERSION_10_15_1 = 101501;

const int MAC_OS_X_VERSION_10_15_4 = 101504;

const int MAC_OS_X_VERSION_10_16 = 101600;

const int MAC_OS_VERSION_11_0 = 110000;

const int MAC_OS_VERSION_11_1 = 110100;

const int MAC_OS_VERSION_11_3 = 110300;

const int MAC_OS_VERSION_11_4 = 110400;

const int MAC_OS_VERSION_11_5 = 110500;

const int MAC_OS_VERSION_11_6 = 110600;

const int MAC_OS_VERSION_12_0 = 120000;

const int MAC_OS_VERSION_12_1 = 120100;

const int MAC_OS_VERSION_12_2 = 120200;

const int MAC_OS_VERSION_12_3 = 120300;

const int MAC_OS_VERSION_12_4 = 120400;

const int MAC_OS_VERSION_12_5 = 120500;

const int MAC_OS_VERSION_12_6 = 120600;

const int MAC_OS_VERSION_12_7 = 120700;

const int MAC_OS_VERSION_13_0 = 130000;

const int MAC_OS_VERSION_13_1 = 130100;

const int MAC_OS_VERSION_13_2 = 130200;

const int MAC_OS_VERSION_13_3 = 130300;

const int MAC_OS_VERSION_13_4 = 130400;

const int MAC_OS_VERSION_13_5 = 130500;

const int MAC_OS_VERSION_13_6 = 130600;

const int MAC_OS_VERSION_13_7 = 130700;

const int MAC_OS_VERSION_14_0 = 140000;

const int MAC_OS_VERSION_14_1 = 140100;

const int MAC_OS_VERSION_14_2 = 140200;

const int MAC_OS_VERSION_14_3 = 140300;

const int MAC_OS_VERSION_14_4 = 140400;

const int MAC_OS_VERSION_14_5 = 140500;

const int MAC_OS_VERSION_14_6 = 140600;

const int MAC_OS_VERSION_14_7 = 140700;

const int MAC_OS_VERSION_15_0 = 150000;

const int MAC_OS_VERSION_15_1 = 150100;

const int MAC_OS_VERSION_15_2 = 150200;

const int MAC_OS_VERSION_15_3 = 150300;

const int MAC_OS_VERSION_15_4 = 150400;

const int MAC_OS_VERSION_15_5 = 150500;

const int MAC_OS_VERSION_15_6 = 150600;

const int MAC_OS_VERSION_16_0 = 160000;

const int MAC_OS_VERSION_26_0 = 260000;

const int MAC_OS_VERSION_26_1 = 260100;

const int __AVAILABILITY_VERSIONS_VERSION_HASH = 93585900;

const String __AVAILABILITY_VERSIONS_VERSION_STRING = 'Local';

const String __AVAILABILITY_FILE = 'AvailabilityVersions.h';

const int __MAC_OS_X_VERSION_MIN_REQUIRED = 150000;

const int __MAC_OS_X_VERSION_MAX_ALLOWED = 260100;

const int __ENABLE_LEGACY_MAC_AVAILABILITY = 1;

const int __DARWIN_WCHAR_MAX = 2147483647;

const int __DARWIN_WCHAR_MIN = -2147483648;

const int __DARWIN_WEOF = -1;

const int _FORTIFY_SOURCE = 2;

const int __DARWIN_NSIG = 32;

const int NSIG = 32;

const int _ARM_SIGNAL_ = 1;

const int SIGHUP = 1;

const int SIGINT = 2;

const int SIGQUIT = 3;

const int SIGILL = 4;

const int SIGTRAP = 5;

const int SIGABRT = 6;

const int SIGIOT = 6;

const int SIGEMT = 7;

const int SIGFPE = 8;

const int SIGKILL = 9;

const int SIGBUS = 10;

const int SIGSEGV = 11;

const int SIGSYS = 12;

const int SIGPIPE = 13;

const int SIGALRM = 14;

const int SIGTERM = 15;

const int SIGURG = 16;

const int SIGSTOP = 17;

const int SIGTSTP = 18;

const int SIGCONT = 19;

const int SIGCHLD = 20;

const int SIGTTIN = 21;

const int SIGTTOU = 22;

const int SIGIO = 23;

const int SIGXCPU = 24;

const int SIGXFSZ = 25;

const int SIGVTALRM = 26;

const int SIGPROF = 27;

const int SIGWINCH = 28;

const int SIGINFO = 29;

const int SIGUSR1 = 30;

const int SIGUSR2 = 31;

const int USER_ADDR_NULL = 0;

const int __DARWIN_OPAQUE_ARM_THREAD_STATE64 = 0;

const int SIGEV_NONE = 0;

const int SIGEV_SIGNAL = 1;

const int SIGEV_THREAD = 3;

const int SIGEV_KEVENT = 4;

const int ILL_NOOP = 0;

const int ILL_ILLOPC = 1;

const int ILL_ILLTRP = 2;

const int ILL_PRVOPC = 3;

const int ILL_ILLOPN = 4;

const int ILL_ILLADR = 5;

const int ILL_PRVREG = 6;

const int ILL_COPROC = 7;

const int ILL_BADSTK = 8;

const int FPE_NOOP = 0;

const int FPE_FLTDIV = 1;

const int FPE_FLTOVF = 2;

const int FPE_FLTUND = 3;

const int FPE_FLTRES = 4;

const int FPE_FLTINV = 5;

const int FPE_FLTSUB = 6;

const int FPE_INTDIV = 7;

const int FPE_INTOVF = 8;

const int SEGV_NOOP = 0;

const int SEGV_MAPERR = 1;

const int SEGV_ACCERR = 2;

const int BUS_NOOP = 0;

const int BUS_ADRALN = 1;

const int BUS_ADRERR = 2;

const int BUS_OBJERR = 3;

const int TRAP_BRKPT = 1;

const int TRAP_TRACE = 2;

const int CLD_NOOP = 0;

const int CLD_EXITED = 1;

const int CLD_KILLED = 2;

const int CLD_DUMPED = 3;

const int CLD_TRAPPED = 4;

const int CLD_STOPPED = 5;

const int CLD_CONTINUED = 6;

const int POLL_IN = 1;

const int POLL_OUT = 2;

const int POLL_MSG = 3;

const int POLL_ERR = 4;

const int POLL_PRI = 5;

const int POLL_HUP = 6;

const int SA_ONSTACK = 1;

const int SA_RESTART = 2;

const int SA_RESETHAND = 4;

const int SA_NOCLDSTOP = 8;

const int SA_NODEFER = 16;

const int SA_NOCLDWAIT = 32;

const int SA_SIGINFO = 64;

const int SA_USERTRAMP = 256;

const int SA_64REGSET = 512;

const int SA_USERSPACE_MASK = 127;

const int SIG_BLOCK = 1;

const int SIG_UNBLOCK = 2;

const int SIG_SETMASK = 3;

const int SI_USER = 65537;

const int SI_QUEUE = 65538;

const int SI_TIMER = 65539;

const int SI_ASYNCIO = 65540;

const int SI_MESGQ = 65541;

const int SS_ONSTACK = 1;

const int SS_DISABLE = 4;

const int MINSIGSTKSZ = 32768;

const int SIGSTKSZ = 131072;

const int SV_ONSTACK = 1;

const int SV_INTERRUPT = 2;

const int SV_RESETHAND = 4;

const int SV_NODEFER = 16;

const int SV_NOCLDSTOP = 8;

const int SV_SIGINFO = 64;

const int PRIO_PROCESS = 0;

const int PRIO_PGRP = 1;

const int PRIO_USER = 2;

const int PRIO_DARWIN_THREAD = 3;

const int PRIO_DARWIN_PROCESS = 4;

const int PRIO_MIN = -20;

const int PRIO_MAX = 20;

const int PRIO_DARWIN_BG = 4096;

const int PRIO_DARWIN_NONUI = 4097;

const int RUSAGE_SELF = 0;

const int RUSAGE_CHILDREN = -1;

const int RUSAGE_INFO_V0 = 0;

const int RUSAGE_INFO_V1 = 1;

const int RUSAGE_INFO_V2 = 2;

const int RUSAGE_INFO_V3 = 3;

const int RUSAGE_INFO_V4 = 4;

const int RUSAGE_INFO_V5 = 5;

const int RUSAGE_INFO_V6 = 6;

const int RUSAGE_INFO_CURRENT = 6;

const int RU_PROC_RUNS_RESLIDE = 1;

const int RLIM_INFINITY = 9223372036854775807;

const int RLIM_SAVED_MAX = 9223372036854775807;

const int RLIM_SAVED_CUR = 9223372036854775807;

const int RLIMIT_CPU = 0;

const int RLIMIT_FSIZE = 1;

const int RLIMIT_DATA = 2;

const int RLIMIT_STACK = 3;

const int RLIMIT_CORE = 4;

const int RLIMIT_AS = 5;

const int RLIMIT_RSS = 5;

const int RLIMIT_MEMLOCK = 6;

const int RLIMIT_NPROC = 7;

const int RLIMIT_NOFILE = 8;

const int RLIM_NLIMITS = 9;

const int _RLIMIT_POSIX_FLAG = 4096;

const int RLIMIT_WAKEUPS_MONITOR = 1;

const int RLIMIT_CPU_USAGE_MONITOR = 2;

const int RLIMIT_THREAD_CPULIMITS = 3;

const int RLIMIT_FOOTPRINT_INTERVAL = 4;

const int WAKEMON_ENABLE = 1;

const int WAKEMON_DISABLE = 2;

const int WAKEMON_GET_PARAMS = 4;

const int WAKEMON_SET_DEFAULTS = 8;

const int WAKEMON_MAKE_FATAL = 16;

const int CPUMON_MAKE_FATAL = 4096;

const int FOOTPRINT_INTERVAL_RESET = 1;

const int IOPOL_TYPE_DISK = 0;

const int IOPOL_TYPE_VFS_ATIME_UPDATES = 2;

const int IOPOL_TYPE_VFS_MATERIALIZE_DATALESS_FILES = 3;

const int IOPOL_TYPE_VFS_STATFS_NO_DATA_VOLUME = 4;

const int IOPOL_TYPE_VFS_TRIGGER_RESOLVE = 5;

const int IOPOL_TYPE_VFS_IGNORE_CONTENT_PROTECTION = 6;

const int IOPOL_TYPE_VFS_IGNORE_PERMISSIONS = 7;

const int IOPOL_TYPE_VFS_SKIP_MTIME_UPDATE = 8;

const int IOPOL_TYPE_VFS_ALLOW_LOW_SPACE_WRITES = 9;

const int IOPOL_TYPE_VFS_DISALLOW_RW_FOR_O_EVTONLY = 10;

const int IOPOL_TYPE_VFS_ENTITLED_RESERVE_ACCESS = 14;

const int IOPOL_SCOPE_PROCESS = 0;

const int IOPOL_SCOPE_THREAD = 1;

const int IOPOL_SCOPE_DARWIN_BG = 2;

const int IOPOL_DEFAULT = 0;

const int IOPOL_IMPORTANT = 1;

const int IOPOL_PASSIVE = 2;

const int IOPOL_THROTTLE = 3;

const int IOPOL_UTILITY = 4;

const int IOPOL_STANDARD = 5;

const int IOPOL_APPLICATION = 5;

const int IOPOL_NORMAL = 1;

const int IOPOL_ATIME_UPDATES_DEFAULT = 0;

const int IOPOL_ATIME_UPDATES_OFF = 1;

const int IOPOL_MATERIALIZE_DATALESS_FILES_DEFAULT = 0;

const int IOPOL_MATERIALIZE_DATALESS_FILES_OFF = 1;

const int IOPOL_MATERIALIZE_DATALESS_FILES_ON = 2;

const int IOPOL_MATERIALIZE_DATALESS_FILES_ORIG = 4;

const int IOPOL_MATERIALIZE_DATALESS_FILES_BASIC_MASK = 3;

const int IOPOL_VFS_STATFS_NO_DATA_VOLUME_DEFAULT = 0;

const int IOPOL_VFS_STATFS_FORCE_NO_DATA_VOLUME = 1;

const int IOPOL_VFS_TRIGGER_RESOLVE_DEFAULT = 0;

const int IOPOL_VFS_TRIGGER_RESOLVE_OFF = 1;

const int IOPOL_VFS_CONTENT_PROTECTION_DEFAULT = 0;

const int IOPOL_VFS_CONTENT_PROTECTION_IGNORE = 1;

const int IOPOL_VFS_IGNORE_PERMISSIONS_OFF = 0;

const int IOPOL_VFS_IGNORE_PERMISSIONS_ON = 1;

const int IOPOL_VFS_SKIP_MTIME_UPDATE_OFF = 0;

const int IOPOL_VFS_SKIP_MTIME_UPDATE_ON = 1;

const int IOPOL_VFS_SKIP_MTIME_UPDATE_IGNORE = 2;

const int IOPOL_VFS_ALLOW_LOW_SPACE_WRITES_OFF = 0;

const int IOPOL_VFS_ALLOW_LOW_SPACE_WRITES_ON = 1;

const int IOPOL_VFS_DISALLOW_RW_FOR_O_EVTONLY_DEFAULT = 0;

const int IOPOL_VFS_DISALLOW_RW_FOR_O_EVTONLY_ON = 1;

const int IOPOL_VFS_NOCACHE_WRITE_FS_BLKSIZE_DEFAULT = 0;

const int IOPOL_VFS_NOCACHE_WRITE_FS_BLKSIZE_ON = 1;

const int IOPOL_VFS_ENTITLED_RESERVE_ACCESS_OFF = 0;

const int IOPOL_VFS_ENTITLED_RESERVE_ACCESS_ON = 1;

const int WNOHANG = 1;

const int WUNTRACED = 2;

const int WCOREFLAG = 128;

const int _WSTOPPED = 127;

const int WEXITED = 4;

const int WSTOPPED = 8;

const int WCONTINUED = 16;

const int WNOWAIT = 32;

const int WAIT_ANY = -1;

const int WAIT_MYPGRP = 0;

const int _QUAD_HIGHWORD = 1;

const int _QUAD_LOWWORD = 0;

const int __DARWIN_LITTLE_ENDIAN = 1234;

const int __DARWIN_BIG_ENDIAN = 4321;

const int __DARWIN_PDP_ENDIAN = 3412;

const int LITTLE_ENDIAN = 1234;

const int BIG_ENDIAN = 4321;

const int PDP_ENDIAN = 3412;

const int __DARWIN_BYTE_ORDER = 1234;

const int BYTE_ORDER = 1234;

const int NULL = 0;

const int EXIT_FAILURE = 1;

const int EXIT_SUCCESS = 0;

const int RAND_MAX = 2147483647;

const int _MALLOC_TYPE_MALLOC_BACKDEPLOY_PUBLIC = 1;

const int kTfLiteOptionalTensor = -1;
